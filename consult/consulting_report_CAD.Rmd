---
title: "Biostatistical Consulting  \n Wastewater-based Epidemiology of COVID-19 in Athens,
  GA, USA 2020-2021"
author: 
  - Cody Dailey [^epibios] & Megan Lott [^ehsc]
  - William Norfolkf ^2^
  - Erin Lipp ^2^
  - Stephen Rathbun ^1^
  
date: "2/11/2021"
output: 
  bookdown::word_document2:
    
    toc: true
    toc_depth: 2
    number_sections: true
editor_options: 
  chunk_output_type: console
---

[^epibios]:Department of Epidemiology and Biostatistics, University of Georgia, Athens, GA, USA
[^ehsc]:Department of Environmental Health, University of Georgia, Athens, GA, USA

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, fig.width = 6, fig.height = 6*9/16, out.width = "100%", out.height = "100%")
```

\newpage
## Common Abbreviations {-}
SARS-CoV-2 = severe acute respiratory syndrome coronavirus 2 (a virus)  
COVID-19 = coronavirus disease 2019 (a disease)  
RT-qPCR = reverse transcription quantitative polymerase chain reaction  

\nextpage
# Preface {-}

This document will serve as a report among collaborators working on the analysis of wastewater surveillance data as part of a biostatistical consulting course (BIOS8200) instructed by Dr. Stephen Rathbun. The wastewater surveillance project was designed and implemented by Dr. Erin Lipp and her environmental health doctoral students, Megan Lott and William Norfolk. Megan Lott acts as the primary "client" for correspondence with lead "consultant" Cody Dailey, under the supervision of Dr. Stephen Rathbun and with meaningful discussion among other course consultants: Nicholas Mallis, Amanda Skarlupka, Morgan Taylor, and Adrianna Westbrook.
  
The report is structured to highlight the analytic workflow from raw data management through analysis techniques. Particular attention is given to analyst options and decision-making to expand on reproducibility. This report will *not* mimic the structure found in scientific manuscripts. Rather and analogously, it will be more detailed and comprehensive *methods* and *results* sections with commentary and coding descriptions. 
  
\newpage
# Background
  
## SARS-CoV-2 and COVID-19 Pandemic
  
Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is the etiological agent of coronavirus disease 2019 (COVID-19). This virus stormed the modern world with a global pandemic that has imposed a gobsmacking burden on society. Direct impacts of deaths, disease, and disability have been met with indirect effects in societal upheaval and economic stress. 
  
Governments, public health professionals, and academics alike were neither expecting of nor prepared for a global emergency of this magnitude. Consequently, there was an obvious dearth of public health and medical capacity. Important to our context, a valid and reliable surveillance system and methodology was missing. We initially had no sound approaches to track the extent of the pandemic, that is, how many and who were infected. 
Even as diagnostic tests were developed and anthropocentric(?) surveillance of cases were reported to responsible parties, there remained obscurity in the underlying pandemic processes. At best infections are only partially observed (and reported) and potential biases from diagnostic accuracy and human behavior loom over our understanding of outbreaks as they happen. Retrospective serological studies can address some of the lapses in surveillance, but do little to address concerns during an ongoing outbreak. 

Dr. Erin Lipp and peers devised ways to monitor the pandemic from another perspective. 
  
## Wastewater-based Epidemiological Surveillance
  
When infected with pathogenic organisms, people will often and unknowingly release or shed the microbes into their environment. This is a fundamental concept in understanding the transmission of communicable infectious disease (e.g., aerosol spread from a cough or sneeze). However, pathogen shedding is not limited to the more dominant / obvious modes of transmission. 
  
Fecal shedding is a well-known and (?) realtively common (?) characteristic of (respiratory?) viral infections and serves as the base rationale for Dr. Lipp's research into SARS-CoV-2 and COVID-19 surveillance in wastewater. The detection and quantification of SARS-CoV-2 ribonucleic acid (RNA) offers a more timely and comprehensive assessment of the extent of an outbreak compared to traditional case reporting systems. 
  
## Objectives
  
With this analysis, we hope to characterize the capabilities of wastewater-based epidemiological surveillance for SARS-CoV-2. In doing so, we outline the following objectives:
  
1. Explore the viral load quantification methodologies
2. Address the partiality and biased nature of COVID-19 case reports
3. Assess a predictive framework using wastewater sampling to inform COVID-19 surveillance

\newpage
# From Feces to Species
  
  
```{r, echo=F, results='hide', message=F, warning=F, error=F}
my.packages <- c("dplyr", "readr", "readxl", "magrittr", "knitr", "openxlsx", "flextable")

sapply(my.packages, library, character.only=T)
rm(my.packages)
```


## Overview of Wastewater Sampling, Processing, and Analysis with Resulting Data

As part of the wastewater surveillance, three Athens-Clarke County (ACC) water reclamation facilities yield water samples twice weekly (on Mondays and Wednesdays)[^frequency]; water samples are collected in triplicate[^sampling] and these redundancies from a particular facility at a particular sampling time point are referred to as biological replicates. Each biological replicate is processed and analyzed via a reverse transcription quantitative polymerase chain reaction (RT-qPCR) -based laboratory workflow. Briefly, viral genetic material (i.e., ribonucleic acid (RNA)) is extracted from the wastewater samples, used as a template to generate complementary deoxyribonucleic acid (DNA) which is then serially amplified to determine the concentration of viral genetic material from the original sample. This RT-qPCR workflow is done both in duplicate for two separate genetic sequence targets (N1 and N2) and, for each sequence target, in triplicate (at least) for additional redundancies[^replicates]. 
  
More comprehensive documentation on data and methodology is found at the University of Georgia (UGA) Center for the Ecology of Infectious Disease (CEID) [COVID-19 Portal: Wastewater Surveillance for SARS-CoV-2 in Athens, GA](https://www.covid19.uga.edu/wastewater-athens.html) and at the [Lipp Laboratory Protocol for Wastewater Surviellence of SARS-CoV-2](https://docs.google.com/document/d/1vvRGrvR5iF3P6d40tKPjUpOZBBKGz_OZAr9Z5xJiCKw/edit). 

[^frequency]:The sampling frequency has varied from the beginning of the study. Initially, samples were taken weekly, but later the sampling frequency was twice weekly as described in the text. However, there remained some irregularity in the sampling frequency.
[^sampling]:Initally, two water samples for each facility were taken, but the study design shifted to collecting three samples for most of the study duration. 
[^replicates]:Overall, the three wastewater reclamation facilities yielded three biological replicates for a single sampling time. The three biological replicates were each further subdivided for RT-qPCR experiments into at least three technical replicates, all of which are duplicated for the different viral sequence targets. So, for any given sampling time, there should be approximately 3x3x3x2=18 RT-qPCR results.
  
\newpage  
# Raw (sewage) Data
  
## RT-qPCR Generated Data
  
Client Megan Lott conducts the RT-qPCR procedures and aggregates raw data output using Microsoft Excel workbooks. These data contain unique identifiers for biological and technical replicates, dates of water sampling and RT-qPCR runs, and the cycle thresholds (ct; i.e., the number of PCR cycles that were needed to detect the fluorescent markers incorporated into the amplified DNA).
  
  
```{r data, echo=T, message=F, warning=F, error=F}
n1 <- read_csv("./data/raw_data/n1_all_cleaned2.csv")
n2 <- read_csv("./data/raw_data/n2_all_cleaned2.csv")
```
  
    
## Reaction Calibration Data
  
In addition to the RT-qPCR runs of the water samples with unknown viral loads, positive controls are run as a means by which to calibrate the laboratory workflow and RT-qPCRs. That is, water samples are "spiked" with known concentrations of viral sequence targets (N1 or N2) or a related sequence (orthologous or paralogous?) such as Bovine Coronovirus genes (not included). These data are needed in the estimation of reaction efficiency and in the calculations that convert RT-qPCR output Ct values to estimations of viral load (e.g., number of viral genetic copies per volume of water sample)
  
  
```{r data2, echo=T, message=F, warning=F, error=F}
qc <- read_csv("./data/raw_data/QC/all_curves.csv")
qc2 <- read_xlsx("./data/raw_data/QC/sarscov2_rna_control.xlsx")
```
    
   
## Wastewater Reclamation Facilities
  
The wastewater reclamation facilities that serve as sources for samples provide data on total influent water / sewage volume (i.e., how much water flows through the facility) and total suspended solids. These characteristics may impact both the water sample and the downstream analysis.

```{r data3, echo=T, message=F, warning=F, error=F}
plant <- read_csv("./data/raw_data/plant_data.csv")
```



## COVID-19 Surveillance Reports
  
The purpose of these efforts in viral load quantification in wastewater is to complement or predict the underlying SARS-CoV-2 infections or COVID-19 epidemic curve. As such, we will also incorporate the COVID-19 case reports from the Georgia Department of Public Health (GaDPH). The data are available as two datasets of COVID-19 case reports. These two datasets are distinguished subtly based on how any particular case report is tied to a specific date. The first dataset contains simple COVID-19 case frequencies for the dates at which the cases were reported. The second dataset contains COVID-19 case frequencies that correspond to a date of symptom onset; however, if a date of symptom onset is not available for a record, then the data are replaced with sample specimen collection. 
  
Although the symptom onset dataset is imperfect, it is likely a useful addition to the analysis at hand. From studies (..examples..), viral sheeding in feces occurs post symptom onset. So, with an epidemic curve relating to the symptom onset potentially *leading* the viral deposition into wastewater and with the, likely, *lagging* epidemic curve for reported cases, we may have some very interesting comparisons a priori. 

Furthermore, we also have data on diagnostic test frequencies and positivity that may shed additional light onto the hidden processes. These data are also given for two date schema, report and specimen collection dates. 

```{r data4, echo=T, message=F, warning=F, error=F}
covid <- read_csv("./consult/01-data/ga_covid_data/epicurve_symptom_date.csv") %>% 
            filter(county=="Clarke") %>% 
            select(symptom.date=`symptom date`, cases, moving_avg_cases)

covid.report <- read_csv("./consult/01-data/ga_covid_data/epicurve_rpt_date.csv") %>% 
            filter(county=="Clarke") %>% 
            select(report_date, cases, moving_avg_cases)

covid.testing <- read_csv("./consult/01-data/ga_covid_data/pcr_positives_col.csv") %>% 
            filter(county=="Clarke") %>% 
            select(collection_date = collection_dt, pcr_tests = `ALL PCR tests performed`, pcr_pos = `All PCR positive tests`)

``` 


\newpage
# Plunging into Data Interrogation
  
  
## Brief Item Analysis

Table \@ref(tab:descript) gives a brief description of the data used in this analysis. Aside from some minor manipulations upon import (e.g., COVID-19 case reports were subset to only include records for Athens Clarke County), the descriptions are for the raw data. From this, we can see some initial data formatting may be necessary. For example, many of the date variables are being read as character strings so they need to be parsed as dates to be useful for analysis. Eventually, the data represented in Table \@ref(tab:descript) will be aggregated / condensed to a single dataframe object used in final analyses. 
  
```{r, descript, echo=F, message=F, warning=F, error=F}
dataset.descriptions <- 
  lapply(
          1:length(ls()), 
          function(.index){
              df <- get(ls(envir=parent.env(environment()))[.index], envir=parent.env(environment())); 
              obs <- nrow(df); 
              vars <- ncol(df); 
              var.names <- paste(names(df), collapse = ",\n"); 
              var.class <- paste(unlist(lapply(df, class)), collapse = ",\n"); 
              
              return(
                data.frame(
                  df=ls(envir=parent.env(environment()))[.index], 
                  obs=obs, 
                  vars=vars, 
                  var.names=var.names, 
                  var.class=var.class)
                )
              }
          ) %>%
  bind_rows() %>%
  setNames(., nm = c("Dataframe Object", "Number of Observations", "Number of Variables", "Variable Names", "Variable Classes"))

dataset.descriptions <- dataset.descriptions[c(4,5,7,8,6,1,2,3),]

dataset.descriptions$`Variable Labels` <- 
  c(
    paste(
      c("PCR Run Date", "PCR Run ID Number", "Sample Collection Date", "Sample Collection Number", "Sample ID", "Sequence Target", "Cycle Threshold", "Slope of Standard Curve", "Y-intercept of Standard Curve", "Copy Number per Microliter of Reaction"), 
      collapse = ",\n"
      ), 
    paste(
      c("PCR Run Date", "PCR Run ID Number", "Sample Collection Date", "Sample Collection Number", "Sample ID", "Sequence Target", "Cycle Threshold", "Slope of Standard Curve", "Y-intercept of Standard Curve", "Copy Number per Microliter of Reaction"), 
      collapse = ",\n"
      ), 
    paste(
      c("Sample Collection Number", "Sequence Target", "Cycle Threshold", "Concentration Quantity Spiked in Sample", "Logarithm Base 10 of Concentration Quantity"), 
      collapse = ",\n"
      ),
    paste(
      c("Sample Collection Number", "Sequence Target", "Cycle Threshold"), 
      collapse = ",\n"
      ), 
    paste(
      c("Date", "Wastewater Reclamation Facility ID", "Volume of Influent Flow in Millions of Gallons", "Total Suspended Solids Concentration in Milligrams per Liter"), 
      collapse=",\n"
      ), 
    paste(
      c("Date of Symptom Onset", "Number of Cases", "Average Number of Cases in Previous 7 Days"), 
      collapse=",\n"
      ), 
    paste(
      c("Date of Report", "Number of Cases", "Average Number of Cases in Previous 7 Days"), 
      collapse=",\n"
      ), 
    paste(
      c("Date of Specimen Collection", "Total PCR Tests Reported or Collected", "Total Positive PCR Tests"), 
      collapse=",\n"
      )
   )


dataset.descriptions$`Dataframe Description` <- 
  c(
    "RT-qPCR Results for N1", 
    "RT-qPCR Results for N2", 
    "Standard Curve Results", 
    "Positive Controls for SARS-CoV-2", 
    "Wastewater Reclamation Facility Processing Data", 
    "COVID-19 Cases by Symptom Onset Date", 
    "COVID-19 Cases by Report Date", 
    "COVID-19 PCR Diagnostics Data"
  )

dataset.descriptions %<>% select(`Dataframe Object`, `Dataframe Description`, everything())

flextable(dataset.descriptions) %>% 
  border_remove() %>% 
  align(part = "header", align = "center") %>%
  align(j=c(1,3,4), align = "center") %>%
  valign(valign = "top") %>% 
  font(part = "all", fontname = "Arial") %>%
  fontsize(part = "all", size = 8) %>% 
  fontsize(part = "header", size = 9) %>% 
  bold(part = "header") %>%
  width(j=7, width = 2.3) %>%
  width(j=6, width = 0.7) %>%
  width(j=5, width = 1.1) %>%
  width(j=c(1,4), width = 0.8) %>%
  width(j=3, width = 1.0) %>%
  width(j=2, width = 0.9) %>%
  border(part = "header", border.bottom = fp_border_default(color = "black", width = 2)) %>%
  border(part = "body", i=1:7, border.bottom = fp_border_default(color = "black", width = 0.5)) %>% 
  set_caption(caption = "Description of Raw Datasets used in Analyses")

```


  
## Initial Data Management and Cleaning (Formatting)

The separate datasets for the N1 and N2 RT-qPCR results are aggregated to a single dataframe to represent all experimental instances. Similarly, the COVID-19 surveillance data are aggregated to a single dataframe as are the positive controls. Each of the variables across all datasets are appropriately formatted and some additional variables are created to aid in structuring (e.g., the wastewater facility IDs are parsed out of the sample IDs). Additionally, some variables are dropped / deleted. These variables will be recreated in later analyses. Finally, the influent flow data from the wastewater reclamation facilities has a unit of a million gallons (assumed US liquid gallon). The influent flow is converted to liters using the unit conversion formula outlined in Equation \@ref(eq:unit-conversion).

\begin{equation}
  1~US~liquid~gallon \times \frac{231~in^3}{1~US~liquid~gallon} \times \frac{0.0254^3~m^3}{1~in^3} \times \frac{1000~L}{1~m^3}
  (\#eq:unit-conversion)
\end{equation}
  
  
```{r manage, echo=F, message=F, warning=F, error=F}
.wbe_old <- bind_rows(n1, n2) %>% 
    mutate(
        run_date=as.Date(run_date, format = "%d-%b-%y"), 
        sample_date=as.Date(sample_date, format = "%d-%b-%y"), 
        facility=substr(sample_id, 1,2), 
        biological_replicate=substr(sample_id, nchar(sample_id), nchar(sample_id)), 
        ct=as.numeric(ifelse(ct=="Undetermined", NA, ct))
    ) %>% 
    arrange(sample_date, facility, target, biological_replicate)


wbe <- bind_rows(n1, n2) %>% 
          mutate(
            run_date=as.Date(run_date, format = "%d-%b-%y"), 
            sample_date=as.Date(sample_date, format = "%d-%b-%y"), 
            facility=substr(sample_id, 1,2), 
            biological_replicate=substr(sample_id, nchar(sample_id), nchar(sample_id)), 
            ct=as.numeric(ifelse(ct=="Undetermined", NA, ct))
            ) %>% 
          arrange(sample_date, facility, target, biological_replicate) %>% 
          select(sample_date, facility, target, biological_replicate, target, collection_num, run_date, run_num, ct)

rm(n1, n2)

qc <- bind_rows(
  qc %>% 
    select(-log_quant) %>% 
    mutate(df="qc1"), 
  qc2 %>% 
    mutate(quantity = 1e8*3/25*2/20, df="qc2") %>% 
    rename(ct=ct_value)
  )
rm(qc2)

plant %<>% mutate(date = as.Date(date, format = "%m/%d/%Y"), influent_flow_L = influent_flow_mg*1e6*231*(0.0254^3)*1000) %>% select(date, wrf, influent_flow_L, influent_tss_mg_l)


.covid_old <- covid

covid <- full_join(
            covid%>%
              select(cases.symptom.onset=cases, date=symptom.date), 
            covid.report%>%
              select(cases.reported=cases, date=report_date), 
            by = "date"
            ) %>% 
         full_join(
           covid.testing%>%
             rename(date=collection_date), 
           by="date"
           ) %>%
         select(date, cases.symptom.onset, cases.reported, pcr_tests, pcr_pos)

rm(covid.report, covid.testing)
rm(dataset.descriptions)
```
  
  
```{r descript2, echo=F, message=F, warning=F, error=F, fig.cap="Table 1. R Objects of Raw Data with Descriptions"}
dataset.descriptions <- 
  lapply(
          1:length(ls()), 
          function(.index){
              df <- get(ls(envir=parent.env(environment()))[.index], envir=parent.env(environment())); 
              obs <- nrow(df); 
              vars <- ncol(df); 
              var.names <- paste(names(df), collapse = ",\n"); 
              var.class <- paste(unlist(lapply(df, class)), collapse = ",\n"); 
              
              return(
                data.frame(
                  df=ls(envir=parent.env(environment()))[.index], 
                  obs=obs, 
                  vars=vars, 
                  var.names=var.names, 
                  var.class=var.class)
                )
              }
          ) %>%
  bind_rows() %>%
  setNames(., nm = c("Dataframe Object", "Number of Observations", "Number of Variables", "Variable Names", "Variable Classes"))

dataset.descriptions <- dataset.descriptions[c(4,3,2,1),]

dataset.descriptions$`Variable Labels` <- 
  c(
    paste(
      c("Sample Collection Date", "Wastewater Reclamation Facility", "Sequence Target", "Biological Replicate ID", "Sample Collection Number", "PCR Run Date", "PCR Run ID Number", "Cycle Threshold"), 
      collapse = ",\n"
      ), 
    paste(
      c("Sample Collection Number", "Sequence Target", "Cycle Threshold", "Concentration Quantity Spiked in Sample", "Dataframe Indicator (two separate before merge)"), 
      collapse = ",\n"
      ), 
    paste(
      c("Date", "Wastewater Reclamation Facility ID", "Volume of Influent Flow in Liters", "Total Suspended Solids Concentration in Milligrams per Liter"), 
      collapse=",\n"
      ), 
    paste(
      c("Date", "Number of Cases with Symptom Onset on the Date", "Number of Cases Reported on the Date", "Total PCR Tests Reported or Collected on the Date", "Total Positive PCR Tests from those Reported or Collected on the Date"), 
      collapse=",\n"
      )
   )


dataset.descriptions$`Dataframe Description` <- 
  c(
    "RT-qPCR Results for All Experiments", 
    "Standard Curves and Positive Controls for SARS-CoV-2", 
    "Wastewater Reclamation Facility Processing Data", 
    "COVID-19 Cases by Symptom Onset Date, Cases by Report Date, and PCR Diagnostics Data"
  )

dataset.descriptions %<>% select(`Dataframe Object`, `Dataframe Description`, everything())

flextable(dataset.descriptions) %>% 
  border_remove() %>% 
  align(part = "header", align = "center") %>%
  align(j=c(1,3,4), align = "center") %>%
  valign(valign = "top") %>% 
  font(part = "all", fontname = "Arial") %>%
  fontsize(part = "all", size = 8) %>% 
  fontsize(part = "header", size = 9) %>% 
  bold(part = "header") %>%
  width(j=7, width = 2.8) %>%
  width(j=6, width = 0.7) %>%
  width(j=5, width = 1.2) %>%
  width(j=c(1,4), width = 0.8) %>%
  width(j=3, width = 1.0) %>%
  width(j=2, width = 0.9) %>%
  border(part = "header", border.bottom = fp_border_default(color = "black", width = 2)) %>%
  border(part = "body", i=1:3, border.bottom = fp_border_default(color = "black", width = 0.5))

```
  

## "Missingness" Evaluation
  
The RT-qPCR workflow can sometimes produce false negative results. That is, despite there being a known concentration of some genetic material in a given sample, it is possible that the RT-qPCR technique can fail to detect any genetic material in the sample. A false negative may occur when the intial concentration of the genetic material in the sample is too small and, despite serial amplifications, this small concentration is never detected. This scenario refers to a concept of a limit of detection; below this limit, i.e., a minute concentration of genetic material, the RT-qPCR technique is insensitive and unable to detect the presence of substrate. As such, it is possible for a given sample to return a null result when processed. From a data perspective, we may consider this as *missing* data.[^missing] It is essential to investigate the extent of this *missingness* within the wastewater samples' PCR results. 

Due to the hierarchical structure in the data[^replicates], we can explore the incomplete data records at various levels of replication. 
  
[^missing]:A more accurate description of this "missingness" in statistical terms would be aligned with the concepts of censoring or truncation. 


```{r, echo=F, message=F, warning=F, error=F}
wbe.summary.tr <- wbe %>% 
                    group_by(sample_date, facility, target, biological_replicate) %>% 
                    summarise(
                      n=n(), 
                      n.miss=sum(is.na(ct)), 
                      ct.mean=mean(ct,na.rm=T), 
                      ct.sd=sd(ct,na.rm=T)
                      ) %>% 
                    mutate_all(function(x){ifelse(is.nan(x), NA, x)}) %>% 
                    ungroup()

wbe.summary.br <- wbe.summary.tr %>%
                    group_by(sample_date, facility, target) %>% 
                    summarise(
                      n.bio = n(),
                      n.bio.non.miss = sum(!is.na(ct.mean)),
                      n.bio.miss = sum(is.na(ct.mean)),
                      n.total = sum(n), 
                      n.total.miss = sum(n.miss), 
                      bio.ct.mean = mean(ct.mean, na.rm = T), 
                      bio.ct.sd = sd(ct.mean, na.rm=T), 
                      tech.ct.dists = paste(paste0(biological_replicate, " = ", round(ct.mean,2), " (sd=", round(ct.sd,2), ", n=", n-n.miss, ")"), collapse = "; ")
                      ) %>%
                      mutate_all(function(x){ifelse(is.nan(x), NA, x)}) %>% 
                      ungroup()

wbe.missing.profile <- wbe.summary.br %>%
                        mutate(p.missing = n.total.miss / n.total) %>%
                        select(sample_date, facility, target, p.missing)

wbe.missing.profile.overall <- wbe.summary.br %>%
                                group_by(sample_date) %>%
                                summarise(p.missing = sum(n.total.miss) / sum(n.total))
wbe.missing.profile.target <- wbe.summary.br %>%
                                group_by(sample_date, target) %>%
                                summarise(p.missing = sum(n.total.miss) / sum(n.total))
wbe.missing.profile.facility <- wbe.summary.br %>%
                                group_by(sample_date, facility) %>%
                                summarise(p.missing = sum(n.total.miss) / sum(n.total))

```


Figures \@ref(fig:all-miss), \@ref(fig:target-miss), and \@ref(fig:facility-miss) show the missing profile of RT-qPCR results for all experimental replicates, stratified by viral sequence target, and stratified by wastewater reclamation facility, respectively. 


```{r all-miss, echo=F, message=F, warning=F, error=F, fig.cap="Overall Profile of Undetermined RT-qPCR Results"}
# layout(matrix(1), widths = lcm(5*2.54), heights = lcm(5*9/16*2.54), respect = T)
par(mar = c(3.1, 3.1, 0, 0), oma = c(0,0,0,0))
plot(wbe.missing.profile.overall$sample_date, wbe.missing.profile.overall$p.missing, type = "o", pch = 16, axes = F, xaxs = 'r', yaxs = 'r', xlab = "", ylab = "", ylim = c(0,1))
all.dates <- unique(wbe.missing.profile.overall$sample_date)
axis(1, at = all.dates[which(c(3, all.dates[2:43]-all.dates[1:42])>2)], labels = F)
axis(1, at = all.dates[which(c(3, all.dates[2:43]-all.dates[1:42])<=2)], labels = F, tck = -0.01)
text(x = all.dates[which(c(3, all.dates[2:43]-all.dates[1:42])>2)], y = par('usr')[3], labels = format(all.dates[which(c(3, all.dates[2:43]-all.dates[1:42])>2)], "%b %d"), srt = 45, adj = c(1.5, 1.5), xpd = T, cex = 0.5)
axis(2, at = seq(0,1, by=0.1), cex.axis = 0.5, las = 1)
axis(2, at = seq(0,1, by=0.05), labels = F, tck = -0.01)
title(xlab = "Sampling Date", ylab = "Proportion of Undetermined RT-qPCR Results", cex.lab = 0.6, line=2)
box()
```





```{r target-miss, echo=F, message=F, warning=F, error=F, fig.cap="Profile of Undetermined RT-qPCR Results by Viral Sequence Target"}
par(mar = c(3.1, 3.1, 0, 0), oma = c(0,0,0,0))
plot(wbe.missing.profile.target$sample_date[which(wbe.missing.profile.target$target=="N1")], wbe.missing.profile.target$p.missing[which(wbe.missing.profile.target$target=="N1")], type = "o", pch = 16, axes = F, xaxs = 'r', yaxs = 'r', xlab = "", ylab = "", ylim = c(0,1))
lines(wbe.missing.profile.target$sample_date[which(wbe.missing.profile.target$target=="N2")], wbe.missing.profile.target$p.missing[which(wbe.missing.profile.target$target=="N2")], lty = 2)
points(wbe.missing.profile.target$sample_date[which(wbe.missing.profile.target$target=="N2")], wbe.missing.profile.target$p.missing[which(wbe.missing.profile.target$target=="N2")], pch = 1)

axis(1, at = all.dates[which(c(3, all.dates[2:43]-all.dates[1:42])>2)], labels = F)
axis(1, at = all.dates[which(c(3, all.dates[2:43]-all.dates[1:42])<=2)], labels = F, tck = -0.01)
text(x = all.dates[which(c(3, all.dates[2:43]-all.dates[1:42])>2)], y = par('usr')[3], labels = format(all.dates[which(c(3, all.dates[2:43]-all.dates[1:42])>2)], "%b %d"), srt = 45, adj = c(1.5, 1.5), xpd = T, cex = 0.5)
axis(2, at = seq(0,1, by=0.1), cex.axis = 0.5, las = 1)
axis(2, at = seq(0,1, by=0.05), labels = F, tck = -0.01)
title(xlab = "Sampling Date", ylab = "Proportion of Undetermined RT-qPCR Results", cex.lab = 0.6, line=2)
box()
legend("bottomleft", lty = c(1,2), pch = c(16,1), legend = c("N1", "N2"), cex = 0.7)
```


```{r facility-miss, echo=F, message=F, warning=F, error=F, fig.cap="Profile of Undetermined RT-qPCR Results by Wastewater Reclamation Facility"}
par(mar = c(3.1, 3.1, 0, 0), oma = c(0,0,0,0))
plot(wbe.missing.profile.facility$sample_date[which(wbe.missing.profile.facility$facility=="CC")], wbe.missing.profile.facility$p.missing[which(wbe.missing.profile.facility$facility=="CC")], type = "o", pch = 0, axes = F, xaxs = 'r', yaxs = 'r', xlab = "", ylab = "", ylim = c(0,1), cex = 0.7)
lines(wbe.missing.profile.facility$sample_date[which(wbe.missing.profile.facility$facility=="NO")], wbe.missing.profile.facility$p.missing[which(wbe.missing.profile.facility$facility=="NO")], lty = 5)
points(wbe.missing.profile.facility$sample_date[which(wbe.missing.profile.facility$facility=="NO")], wbe.missing.profile.facility$p.missing[which(wbe.missing.profile.facility$facility=="NO")], pch = 2, cex = 0.7)
lines(wbe.missing.profile.facility$sample_date[which(wbe.missing.profile.facility$facility=="MI")], wbe.missing.profile.facility$p.missing[which(wbe.missing.profile.facility$facility=="MI")], lty = 3)
points(wbe.missing.profile.facility$sample_date[which(wbe.missing.profile.facility$facility=="MI")], wbe.missing.profile.facility$p.missing[which(wbe.missing.profile.facility$facility=="MI")], pch = 8, cex = 0.7)


axis(1, at = all.dates[which(c(3, all.dates[2:43]-all.dates[1:42])>2)], labels = F)
axis(1, at = all.dates[which(c(3, all.dates[2:43]-all.dates[1:42])<=2)], labels = F, tck = -0.01)
text(x = all.dates[which(c(3, all.dates[2:43]-all.dates[1:42])>2)], y = par('usr')[3], labels = format(all.dates[which(c(3, all.dates[2:43]-all.dates[1:42])>2)], "%b %d"), srt = 45, adj = c(1.5, 1.5), xpd = T, cex = 0.5)
axis(2, at = seq(0,1, by=0.1), cex.axis = 0.5, las = 1)
axis(2, at = seq(0,1, by=0.05), labels = F, tck = -0.01)
title(xlab = "Sampling Date", ylab = "Proportion of Undetermined RT-qPCR Results", cex.lab = 0.6, line=2)
box()
legend("bottomleft", lty = c(1,5,3), pch = c(0,2,8), legend = c("CC", "NO", "MI"), cex = 0.7)

```

```{r, echo=F, message=F, warning=F, error=F}

wbe.missing.profile.overall.samples <- wbe.summary.br %>%
    group_by(sample_date) %>%
    summarise(n.samples.non.miss = sum(n.total) - sum(n.total.miss))

```



The "missingness" is quite extensive. In fact, of the total `r nrow(wbe)` experimental units, there are `r paste0(sum(is.na(wbe$ct)), " (", round(sum(is.na(wbe$ct))/nrow(wbe)*100, 1), "%)")` with undetermined values for the cycle threshold, i.e., effectively missing or censored. On the other hand, Figure \@ref(fig:n-non-miss) shows the total number of samples that RT-qPCR detected as positive. There are an average of `r mean(wbe.missing.profile.overall.samples$n.samples.non.miss)` positive samples for each day across the study; over 80% of the sampling dates have more than 6 positive results. [^caveat]
  
[^caveat]:The number of experiments ran / technical replicates was not strictly uniform across the study. 

```{r n-non-miss, echo=F, message=F, warning=F, error=F, fig.cap="Overall Profile of Positive Samples from RT-qPCR"}
# layout(matrix(1), widths = lcm(5*2.54), heights = lcm(5*9/16*2.54), respect = T)
par(mar = c(3.1, 3.1, 0, 0), oma = c(0,0,0,0))
plot(wbe.missing.profile.overall.samples$sample_date, wbe.missing.profile.overall.samples$n.samples.non.miss, type = "o", pch = 16, axes = F, xaxs = 'r', yaxs = 'r', xlab = "", ylab = "", ylim = c(0, max(wbe.missing.profile.overall.samples$n.samples.non.miss)*1.1))
all.dates <- unique(wbe.missing.profile.overall$sample_date)
axis(1, at = all.dates[which(c(3, all.dates[2:43]-all.dates[1:42])>2)], labels = F)
axis(1, at = all.dates[which(c(3, all.dates[2:43]-all.dates[1:42])<=2)], labels = F, tck = -0.01)
text(x = all.dates[which(c(3, all.dates[2:43]-all.dates[1:42])>2)], y = par('usr')[3], labels = format(all.dates[which(c(3, all.dates[2:43]-all.dates[1:42])>2)], "%b %d"), srt = 45, adj = c(1.5, 1.5), xpd = T, cex = 0.5)
axis(2, at = seq(0,max(wbe.missing.profile.overall.samples$n.samples.non.miss)*1.1, by=5), cex.axis = 0.5, las = 1)
axis(2, at = seq(0,max(wbe.missing.profile.overall.samples$n.samples.non.miss)*1.1, by=1), labels = F, tck = -0.01)
title(xlab = "Sampling Date", ylab = "Number of Samples with Detectable Viral Loads", cex.lab = 0.6, line=2)
box()
```


  
  
Given the hierarchical structure of the data, it may be useful to explore the undetermined values at different levels. Table \@ref(tab:prop-detect) and Figure \@ref(fig:bar-prop-detect) show the percentages / proportions of undetermined values stratified by wastewater reclamation facility and viral sequence target; these data are aggregated across the study duration. The three columns of percentages (with frequencies in parentheses) are given to show the three levels of organization within the data.   
  
```{r, prop-detect, echo = F, message=F, warning=F, error=F}
prop.detect <- wbe.summary.br%>%
                      group_by(facility, target)%>%
                      summarise(
                        n.miss.day=sum(n.bio.non.miss==0),
                        n.days=n(),
                        n.bio.non.miss=sum(n.bio.non.miss), 
                        n.bio.miss=sum(n.bio.miss), 
                        n.bio=sum(n.bio), 
                        n.tech.miss=sum(n.total.miss), 
                        n.tech=sum(n.total)
                        )%>%
                      mutate(
                        prop.m.day=n.miss.day/n.days,
                        prop.m.bio=n.bio.miss/n.bio, 
                        prop.m.tech=n.tech.miss/n.tech
                        )

prop.detect.tab <- prop.detect %>%
  mutate(
    prop.day = paste0(round(prop.m.day*100,1), " (", n.miss.day, " / ", n.days, ")"),
    prop.bio = paste0(round(prop.m.bio*100,1), " (", n.bio.miss, " / ", n.bio, ")"), 
    prop.tech = paste0(round(prop.m.tech*100,1), " (", n.tech.miss, " / ", n.tech, ")"))%>%
  select(facility, target, prop.day, prop.bio, prop.tech)

names(prop.detect.tab) <- c("Wastewater Reclamation Facility", "Viral Sequence Target", "Percentage of Sampling Dates with All Undetermined Results", "Percentage of Biological Replicates with All Undetermined Results", "Percentage of Technical Replicates with Undetermined Results")

flextable(prop.detect.tab) %>% 
  font(part = "all", fontname = "Arial") %>%
  fontsize(part = "header", size = 9) %>%
  fontsize(part = "body", size = 8) %>%
  border_remove() %>%
  border(part = "header", border.bottom = fp_border_default(color = "black", width = 2)) %>%
  width(j=1:2, width = 1) %>%
  width(j=3:5, width = 1.5) %>%
  merge_v(j=1) %>%
  valign(j=1, valign = "top") %>%
  bold(part = "header") %>% 
  set_caption(caption = "Extent of Undetermined RT-qPCR Values at Various Levels of the Data Hierarchy")
```

  
The first column of percentages labeled "Percentage of Sampling Dates with All Undetermined Results" in Table \@ref(tab:prop-detect) shows the frequencies of undetermined RT-qPCR values with respect to the date of sampling. To explain further, there are `r length(unique(wbe$sample_date))` distinct sampling dates in the data[^other-missing]. So, for a given wastewater reclamation facility and viral sequence target, there are some sampling dates where ***every*** RT-qPCR experiment for that day's samples yielded undetermined results. For example, the samples from Cedar Creek on 30 June 2020 all yielded undetermined results for both viral sequence targets, N1 and N2, in the RT-qPCR experiments. The column labeled "Percentage of Sampling Dates with All Undetermined Results" in Table \@ref(tab:prop-detect) shows these frequencies in the dataset. 
    
    
[^other-missing]:There are some implicitly missing data within the datasets with respect to sampling dates, facilities, and sequence targets. That is, there are some instances where no RT-qPCR results exist in the data for a given sampling date, facility, and viral sequence target. For example, there are no results Cedar Creek on 14 July 2020 for either viral sequence target, N1 or N2.  
  
```{r, echo = F, message=F, warning=F, error=F}
n.bio.reps <- wbe %>% 
                group_by(sample_date, facility, biological_replicate) %>% 
                summarise(n=1) %>% 
                group_by(facility) %>% 
                summarise(n=n())


# table(wbe$biological_replicate, wbe$target, wbe$sample_date, wbe$facility)

## potential data entry error???
# table(wbe$biological_replicate[which(wbe$sample_date%in%c(as.Date("2020-11-18"), as.Date("2020-11-19")) & wbe$facility=="CC")], wbe$target[which(wbe$sample_date%in%c(as.Date("2020-11-18"), as.Date("2020-11-19")) & wbe$facility=="CC")], wbe$sample_date[which(wbe$sample_date%in%c(as.Date("2020-11-18"), as.Date("2020-11-19")) & wbe$facility=="CC")])
```
  
The next column, labeled "Percentage of Biological Replicates with All Undetermined Results" goes to the next smaller level in the data hierarchy: the biological replicates. As a quick reminder, the biological replicates refer to the (approximately) three individual composite water / sewage samples from a single wastewater reclamation facility on a single sampling date. There are a total of `r n.bio.reps$n[which(n.bio.reps$facility=="CC")]`, `r n.bio.reps$n[which(n.bio.reps$facility=="MI")]`, and `r n.bio.reps$n[which(n.bio.reps$facility=="NO")]` biological replicates from the Cedar Creek, Middle Oconee, and North Oconee Wastewater Reclamation Facilities, respectively.[^potential-errors] So, the percentages in this column refer to the frequencies of *all* the technical replicate experiments for any one biological replicate yielding undetermined results.   
  
The last column in Table \@ref(tab:prop-detect) shows the percentage of all technical replicates (i.e., the smallest experimental unit) that yielded an undetermined result from the RT-qPCR. Unlike the other columns, this one is a simple frequency calculation of the raw data. In fact, if you sum the denominators of each row in the last column, then you will get `r nrow(wbe)` which is the total number of rows / experimental units in the RT-qPCR dataset. 
  

[^potential-errors]:Upon closer examination to the data, I may have uncovered a potential data entry error. For the Cedar Creek samples, there are uncrossed biological replicates with respect to viral sequence targets. For example, there are 3 technical replicates for each of 3 biological replicates for Cedar Creek on 18 November 2020; however, these records only exist for the N1 sequence target. Similarly, there are 9 records for Cedar Creek on 19 November 2020, but these are only for the N2 sequence target. I suspect these experimental units are from the same biological samples collected on the same day and that the 18 v 19 date is a simple entry error. There may be other entry errors, though. Closer inspection of the x-axes for Figures \@ref(fig:all-miss), \@ref(fig:target-miss), and \@ref(fig:facility-miss) may shed some light on the matter. 



```{r bar-prop-detect, echo = F, message=F, warning=F, error=F, fig.cap = "Barplot of Proportions Undetermined"}
par(mar=c(5.1,4.1,4.1,2.1))
col.indices <- c((0:1295*6+1)[0:215*6+1][0:35*6+1][0:5*6+1], (0:1295*6+1)[0:215*6+1][0:35*6+1][0:5*6+1]+216, (0:1295*6+1)[0:215*6+1][0:35*6+1][0:5*6+1]+432)
col.indices <- col.indices[order(col.indices)]

bar.data <- t(prop.detect[,10:12])

barplot(matrix(1, nrow=3, ncol=6), beside = T, col = viridis::viridis(max(col.indices), alpha = 0.25)[col.indices], cex.axis = 0.7, las = 1)
barplot(bar.data, beside = T, add = T, col = viridis::viridis(max(col.indices))[col.indices], legend=F, axes = F)

text(x=seq(1,{18+5})[-seq(4,{18+5}, by = 4)]+0.5, y=0, labels = c("D", "B", "T"), xpd=T, adj = c(0.5, 1), cex=0.5)
text(x=seq(1,{18+5})[-seq(4,{18+5}, by = 4)][seq(2,23,by=3)]+0.5, y=0, labels = rep(c("N1", "N2"), 3), xpd=T, adj = c(0.5, 3), cex=0.6)
axis(1, at = c(2.5,6.5), tick = T, labels = F, line = 2, tck = 0.02)
axis(1, at = c(10.5,14.5), tick = T, labels = F, line = 2, tck = 0.02)
axis(1, at = c(18.5,22.5), tick = T, labels = F, line = 2, tck = 0.02)
text(x=seq(4,{18+5}, by=8)+0.5, y=0, labels = c("CC", "MI", "NO"), xpd=T, adj = c(0.5, 6), cex=0.7)
title(ylab = "Proportion Undetermined Values")
```


To further explore the missingness and observed differences with respect to the viral sequence target and wastewater reclamation facility, logistic regression models were fit to the data. That is, a generalized linear model was fit to undetermined RT-qPCR values using viral sequence targets and wastewater reclamation facilities as predictors.[^binomial] Table \@ref(tab:fit-tab) shows extensive results of these logistic regression models. 
  
[^binomial]:The logistic regression models used the cbind() designation of a binomial for the response variable. The models were fit on the data used to generate Table \@ref(tab:prop-detect). For example, the frequency of undetermined results aggregated to the sampling date for Cedar Creek and the N1 sequence target is 9 out of 41 total sampling dates. So, the model used the "missing" values and "non-missing" values to form the response, e.g., `cbind(9, 32)` for Cedar Creek and the N1 sequence target. This binomial designation is equivalent in estimation as the more common 0 / 1 coding in logistic regression. 
  
```{r, echo=F, message=F, warning=F, error=F}

fit.day <- glm(cbind(n.miss.day, n.days-n.miss.day)~target+facility, data=prop.detect, family="binomial")
fit.day.aov <- car::Anova(fit.day, type = 3)
fit.day.mean.ests <- predict(fit.day, type = "response", se.fit = T)
fit.day.mean.ests <- paste0(round(fit.day.mean.ests$fit,3), " (", round(fit.day.mean.ests$fit - fit.day.mean.ests$se.fit*qnorm(0.975),3), ", ", round(fit.day.mean.ests$fit + fit.day.mean.ests$se.fit*qnorm(0.975), 3), ")")



fit.bio <- glm(cbind(n.bio.miss, n.bio.non.miss)~target+facility, data=prop.detect, family="binomial")
fit.bio.aov <- car::Anova(fit.bio, type = 3)
fit.bio.mean.ests <- predict(fit.bio, type = "response", se.fit = T)
fit.bio.mean.ests <- paste0(round(fit.bio.mean.ests$fit,3), " (",round(fit.bio.mean.ests$fit - fit.bio.mean.ests$se.fit*qnorm(0.975),3), ", ", round(fit.bio.mean.ests$fit + fit.bio.mean.ests$se.fit*qnorm(0.975), 3), ")")


fit.tech <- glm(cbind(n.tech.miss, n.tech-n.tech.miss)~target+facility, data=prop.detect, family="binomial")
fit.tech.aov <- car::Anova(fit.tech, type = 3)
fit.tech.mean.ests <- predict(fit.tech, type = "response", se.fit = T)
fit.tech.mean.ests <- paste0(round(fit.tech.mean.ests$fit,3), " (", round(fit.tech.mean.ests$fit - fit.tech.mean.ests$se.fit*qnorm(0.975),3), ", ", round(fit.tech.mean.ests$fit + fit.tech.mean.ests$se.fit*qnorm(0.975), 3), ")")

fit.tab <- cbind(prop.detect.tab[,1:2], "P of Sampling bio with All Undetermined Results"=fit.bio.mean.ests, "P of Biological Replicates with All Undetermined Results"=fit.bio.mean.ests, "P of Technical Replicates with Undetermined Results"=fit.tech.mean.ests)


fit.aovs <- rbind(cbind(Model = "Days", Effect=rownames(fit.day.aov), round(fit.day.aov, 3)), cbind(Model = "Bios", Effect=rownames(fit.bio.aov), round(fit.bio.aov, 3)), cbind(Model = "Techs", Effect=rownames(fit.tech.aov), round(fit.tech.aov,3)))
fit.aovs %<>% add_row(.before=1) %>% mutate_all(as.character)
fit.aovs[1,] <- as.list(names(fit.aovs))


fit.tab <- add_row(fit.tab %>% ungroup(), .before = 1) %>% mutate_all(as.character)
fit.tab[1,] <- as.list(names(fit.tab))
fit.tab <- add_row(fit.tab)
# nrow(fit.tab)
i=1
repeat{
  fit.tab <- add_row(fit.tab)
  fit.tab[nrow(fit.tab),] <- as.list(fit.aovs[i,])
  if(nrow(fit.tab)==6+nrow(fit.aovs)+2){break}else{i=i+1}

}


fit.day.coefs <- cbind(Model = "Days", Parameter=c("N2 v N1", "MI v CC", "NO v CC"), "OR Estimate"=paste0(round(exp(coef(fit.day)[-1]), 3), " (", round(exp(confint(fit.day)[-1,1]),3), ", ", round(exp(confint(fit.day)[-1,2]),3), ")"))
fit.bio.coefs <- cbind(Model = "Bios", Parameter=c("N2 v N1", "MI v CC", "NO v CC"), "OR Estimate"=paste0(round(exp(coef(fit.bio)[-1]), 3), " (", round(exp(confint(fit.bio)[-1,1]),3), ", ", round(exp(confint(fit.bio)[-1,2]),3), ")"))
fit.tech.coefs <- cbind(Model = "Techs", Parameter=c("N2 v N1", "MI v CC", "NO v CC"), "OR Estimate"=paste0(round(exp(coef(fit.tech)[-1]), 3), " (", round(exp(confint(fit.tech)[-1,1]),3), ", ", round(exp(confint(fit.tech)[-1,2]),3), ")"))


fit.coefs <- rbind(fit.day.coefs, fit.bio.coefs, fit.tech.coefs)
fit.coefs <- fit.coefs %>% as.data.frame() %>% add_row(.before = 1)
fit.coefs[1,] <- as.list(names(fit.coefs))
fit.coefs <- cbind(fit.coefs, NA, NA)

fit.tab <- add_row(fit.tab)
# nrow(fit.tab)
i=1
repeat{
  fit.tab <- add_row(fit.tab)
  fit.tab[nrow(fit.tab),] <- as.list(fit.coefs[i,])
  if(nrow(fit.tab)==16+nrow(fit.coefs)){break}else{i=i+1}

}
```

```{r fit-tab, echo = F, message=F, warning=F, error=F}

flextable(fit.tab) %>%
  delete_part(part = "header") %>%
  border_remove() %>% 
  
  border(part = "body", i = which(fit.tab$`Wastewater Reclamation Facility`=="Model")-1, border.top = fp_border_default(color="black", width = 0.5)) %>% 
  border(part = "body", i = c(1,which(fit.tab$`Wastewater Reclamation Facility`=="Model")), border.bottom = fp_border_default(color="black", width = 2)) %>%
  font(part = "all", fontname = "Arial") %>% 
  fontsize(part = "header", size = 9) %>%
  fontsize(part = "body", size = 8) %>% 
  fontsize(part = "body", i = c(1,which(fit.tab$`Wastewater Reclamation Facility`=="Model")), size = 9) %>%
  bold(part = "header") %>% 
  bold(part = "body", i = c(1,which(fit.tab$`Wastewater Reclamation Facility`=="Model"))) %>%
  merge_v(j=1) %>% 
  width(j=1:2, width = 1) %>%
  width(j=3:5, width = 1.5) %>%
  align(part = "header", align = "center") %>%
  align(part = "body", j=3:5, align = "right") %>% 
  align(part = "body", i = c(1,which(fit.tab$`Wastewater Reclamation Facility`=="Model")), align = "center") %>% 
  valign(part = "body", j=1, valign = "top") %>%
  bold(part = "body", j=5, i = c(13,15)) %>% 
  bold(part = "body", j=3, i = c(22,25)) %>% 
  italic(part = "body", j=3, i = 26) %>%
  set_caption(caption = "Logistic Regression for Undetermined RT-qPCR Results on Viral Sequence Target and Wastewater Reclamation Facility") %>% 
  compose(part="body", j=3, i=1, value = as_paragraph("P\U0302 of Sampling Days with All Undetermined Results")) %>% 
  compose(part="body", j=4, i=1, value = as_paragraph("P\U0302 of Biological Replicates with All Undetermined Results")) %>%
  compose(part="body", j=5, i=1, value = as_paragraph("P\U0302 of Technical Replicates with All Undetermined Results"))
```

Table \@ref(tab:fit-tab) is essentially three tables in one. From top to bottom, it includes mean probability estimates from the logistic regression models for the missing frequencies at the previously discussed data hierarchies, analyses of deviance, and model parameter estimates. Interestingly, the facility was determined to have had some effect on the frequencies of undetermined results for the data at the biological replicate level and the technical replicate level after controlling for sequence target; no significant differences were observed for the data aggregated at the sampling date level, but this is not surprising as the artificial reduction in sample size likely greatly reduced the power of the analysis. The viral sequence target does not exhibit evidence of having any impact on "missing" data; that is, the frequencies of undetermined experimental values are indistinguishable between N1 and N2 at each level while controlling for facility. The differences among facilities appears to be mostly the result of lower frequencies (lower estimated odds) of undetermined values for the Middle Oconee wastewater reclamation facility when compared with Cedar Creek. Perhaps notable, the estimated odds ratio comparing undetermined values in North Oconee to Cedar Creek is *marginally* significant (i.e., its confidence interval just barely includes the null value). All to say, it seems that Cedar Creek samples are more likely to yield undetermined results from the RT-qPCR experiments.   
  
Regardless of the ***patterns*** of missingess within the data, it may be important to explore approaches to use any information of a missing / undetermined result. How could we accomplish this? Perhaps, incorporation of limits of detection may help.   
  




## Exploring Limits of Detection and Quantification from the Data Perspective
  
The limit of detection refers to a threshold where a positive sample (i.e., one that **does** contain targeted substrate) is indistinguishable from a negative sample (i.e., one that **does not** contain any targeted substrate) given some technique. Therefore, some of the undetermined values within the data could be truly positive, but, since the substrate is at such a low concentration, the experimental approach fails to detect anything (i.e., a false negative).[^other-miss-source]

[^other-miss-source]:It may be important to acknowledge that these experimental methods in themselves are not without flaw nor devoid of other potential sources of measurement error. So, even given a sample with substrate concentrations above the limit of detection, it is still possible and not unlikely that an experiment could have an undetermined / negative result (i.e., spurious). 

Theoretically, it may be possible to calculate a limit of detection through considerations of the experimental procedure alone.[^lower-bound] The RT-qPCR workflow begins with the water / sewage sample and 280 microliters, $\mu L$, is extracted and used for each technical replicate experimental procedure. Following, the RNA is eluted into 60 $\mu L$ of a buffer solution. Three $\mu L$ of the 60 $\mu L$ of RNA-in-buffer is extracted and added to a reverse transcription reaction of 25 $\mu L$. Finally, 2 $\mu L$ of the reverse transcription reaction product is transferred to 20 $\mu L$ wells.[^description-flaws]   
  
Altogether, these quantities are important for understanding the dilution scheme imposed by the lab procedures which can then be used in calculating the theoretical limit of detection (Equation \@ref(eq:dilution)). In words, if there was a ***single*** viral sequence copy within our sample, then we can track the copy through the dilution to calculate the concentration that used in the PCR amplifications. 

\begin{equation}
  Theoretical~Limit~of~Detection = \frac{1~viral~copy}{60\mu L} \times \frac{3\mu L}{25\mu L} \times \frac{2\mu L}{20\mu L} = 0.0002~copies \mu L^{-1}
  (\#eq:dilution)
\end{equation}
  
Although a useful quantity / concentration to know, this limit of detection is rather optimistic. So, in lieu of an experimental approach, we investigated some data analysis approaches to quantify the limit. 
    
[^lower-bound]:This theoretical limit of detection may be more aligned with an absolute lower bound for procedure as it would assume perfectly efficient lab techniques and reactions. 
[^description-flaws]:I am not confident in my verbose description of the lab procedure nor essential verbage. For example, I cannot wrap my head around how 25 $\mu L$ from the reverse transcription reaction can be used for 2 $\mu L$ aliquots, unless 1 $\mu L$ is discarded / excess. 
  
    
    
### Observed Distributions of Cycle Thresholds and Normality
    
The cycle threshold values for detected samples range from `r min(wbe$ct, na.rm=T)` to `r max(wbe$ct, na.rm=T)`; a histogram of these data is shown in Figure \@ref(fig:ct-hist). There appears to be a stacking of observations around Ct = 37. This potential "boundary" in the data distribution may indicate a limit of detection. 

```{r ct-hist, echo = F, message = F, warning=F, error=F, fig.cap="Histogram of Cycle Thresholds"}
par(mar = c(2.1, 2.1, 1.1, 0))

hist(wbe$ct, breaks = 100, axes = F, xlab = "", ylab = "", main = "")
axis(1, at = 32:39, labels = T, tick = T, cex.axis = 0.7, line = 0, padj = -1.75, tck = -0.0125)
title(xlab = "Cycle Threshold", line = 1)
axis(2, at = seq(0,40,by=10), tick = T, labels = T, cex.axis = 0.7, line = 0, hadj = 0.25, tck = -0.0125, las = 1)
title(ylab = "Frequency", line = 1.25)
# title(main = "Histogram of Cycle Thresholds", line = 0.25)
```
  
Figure \@ref(fig:qq-plots-ct) shows the normal quantile plots for the cycle thresholds of the two viral sequence targets, N1 and N2, separately. If the data were approximately normally distributed, we could expect a roughly straight line (shown by the gray diagonal line). However, there appears to be some deviations from normality at the upper tail for higher Ct values. From this normal quantile plots, we can identify two points of interest. The first point of interest would be the initial inflection point (scanning from left to right or low Ct values to higher Ct values) where the data deviate from a normal distribution tail. The second point of interest would approximate to the point of truncation seen in Figure \@ref(fig:ct-hist) and in Figure \@ref(fig:qq-plots-ct) as the right-hand side of the flatter portion of the "curve" of data points.     
  

```{r qq-plots-ct, echo=F, message=F, warning=F, error=F, fig.height=6*9/16*2, fig.cap="Normal Quantile-Quantile Plots of Observed Cycle Thresholds for Viral Sequence Targets N1 and N2"}

qqnorm.ct.n1 <- qqnorm(wbe$ct[which(wbe$target=="N1")], plot.it = F) %>% as.data.frame()
qqnorm.ct.n2 <- qqnorm(wbe$ct[which(wbe$target=="N2")], plot.it = F) %>% as.data.frame()


qqnorm.Explorer.ct <- function(qqnorm.ct){
        qqnorm.ct <- qqnorm.ct[which(complete.cases(qqnorm.ct)),]
        qqnorm.ct <- qqnorm.ct[order(qqnorm.ct$x),]
        qqnorm.ct <- cbind(qqnorm.ct, rbind(NA, qqnorm.ct[-nrow(qqnorm.ct),])) %>% setNames(., nm = c("x", "y", "x-1", "y-1"))
        qqnorm.ct %<>% mutate(rise = y-`y-1`, run = x-`x-1`) %>% mutate(slope = rise / run)
        
        qqnorm.ct$lod <- NA
        qqnorm.ct$loq <- NA
        
        prev.slope <- 1
        lod.found <- 0
        for(i in nrow(qqnorm.ct):2){
          if(lod.found==0){
            if(qqnorm.ct$slope[i]<1 & prev.slope <1){
              qqnorm.ct$lod[i] <- 1
              lod.found <- 1
            }else{
              prev.slope <- qqnorm.ct$slope[i]
            }
          }
          if(lod.found==1){
            if(qqnorm.ct$slope[i]>1){
              qqnorm.ct$loq[i] <- 1
              break
            }else{
              prev.slope <- qqnorm.ct$slope[i]
            }
          }
        }
        
        
        lod.ct <- qqnorm.ct$y[which(qqnorm.ct$lod==1)]
        loq.ct <- qqnorm.ct$y[which(qqnorm.ct$loq==1)]

        return(list(qqnorm.dataset = qqnorm.ct, lod = lod.ct, loq = loq.ct))
}



qqnorm.ct.n1 <- qqnorm.Explorer.ct(qqnorm.ct.n1)
qqnorm.ct.n2 <- qqnorm.Explorer.ct(qqnorm.ct.n2)
        


# png(filename = "./consult/03-output/limits_ct.png", height = 9, width = 8, units = "in", res = 300)
# layout(matrix(c(1,2), nrow = 2), widths = c(lcm(6*2.54)), heights = c(lcm(6*9/16*2.54), lcm(6*9/16*2.54)))
# par(mar = c(2.1, 2.1, 1.1, 0), fig = c(0,1,0.5,1))
# par(mar = c(2.1, 2.1, 1.1, 0), fig = c(0,1,0,0.5), new = T)
# 
# dev.off()


par(mfcol = c(2,1), mar = c(2.1, 2.1, 1.1, 0))

# layout.show(2)
qqnorm(wbe$ct[which(wbe$target=="N1")],  axes = F, ylab = "", xlab = "", main = "")
qqline(wbe$ct[which(wbe$target=="N1")], col = "gainsboro")
axis(1, at = -3:3, tick = T, labels = T, cex.axis = 0.7, line = 0, padj = -1.75, tck = -0.0125)
title(xlab = "Theoretical Quantiles", line = 1)

axis(2, at = 32:39, tick = T, labels = T, cex.axis = 0.7, line = 0, hadj = 0.25, tck = -0.0125, las = 1)
title(ylab = "Observed Cyle Thresholds", line = 1.25)

abline(h = qqnorm.ct.n1$lod)
text(par('usr')[1], par('usr')[4], labels = paste("LOD =", round(qqnorm.ct.n1$lod,3)), adj = c(-0.05,1.2))
abline(h = qqnorm.ct.n1$loq, lty = 3)
text(par('usr')[1], par('usr')[4], labels = paste("LOQ =", round(qqnorm.ct.n1$loq,3)), adj = c(-0.05,2.4))
legend("bottomright", lty = c(1,3), legend = c("LOD", "LOQ"))
title(main = "Normal Q-Q Plot for N1 Cycle Threshold", line = 0.25)
box()


text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "A", adj = c(0,1), xpd = T, cex = 1, font = 2)




qqnorm(wbe$ct[which(wbe$target=="N2")],  axes = F, ylab = "", xlab = "", main = "")
qqline(wbe$ct[which(wbe$target=="N2")], col = "gainsboro")
axis(1, at = -3:3, tick = T, labels = T, cex.axis = 0.7, line = 0, padj = -1.75, tck = -0.0125)
title(xlab = "Theoretical Quantiles", line = 1)

axis(2, at = 33:39, tick = T, labels = T, cex.axis = 0.7, line = 0, hadj = 0.25, tck = -0.0125, las = 1)
title(ylab = "Observed Cyle Thresholds", line = 1.25)


abline(h = qqnorm.ct.n2$lod)
text(par('usr')[1], par('usr')[4], labels = paste("LOD =", round(qqnorm.ct.n2$lod,3)), adj = c(-0.05,1.2))
abline(h = qqnorm.ct.n2$loq, lty = 3)
text(par('usr')[1], par('usr')[4], labels = paste("LOQ =", round(qqnorm.ct.n2$loq,3)), adj = c(-0.05,2.4))
legend("bottomright", lty = c(1,3), legend = c("LOD", "LOQ"))
title(main = "Normal Q-Q Plot for N2 Cycle Threshold", line = 0.25)
box()
text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "B", adj = c(0,1), xpd = T, cex = 1, font = 2)

# dev.off()

```

So, with careful consideration, we can rationalize the meaning of these points. The point of truncation within the histogram and the larger point of interest determined from the normal quantile plots may serve as a proxy for the limit of detection. There are few values more extreme than this cutoff point and it seemingly acts as a boundary in the Ct values distribution.   
  
Also, the point of inflection where the data deviate from normality may serve as a proxy for a limit of quantification. Similar to a limit of detection, a limit of quantification represents a lower boundary of concentrations / quantities where the experimental procedure performs poorly. The limit of quantification is defined for higher concentrations / larger quantities of substrate when compared to the limit of detection. This means that the experimental procedure would be able to detect a sample as positive as it is above the detection limit. However, for those samples with concentrations above the limit of detection yet below the limit of quantification, the experiment fails to accurately estimate and distinguish the concentrations of substrate.   
  
Table \@ref(tab:limits-table) explores where the cycle threshold data fall with respect to these newly defined limits. This table is three separate tables (A, B, C) stacked on top of one another showing summaries at the different hierarchy levels within the data: A = technical replicates; B = biological replicates; and, C = sampling days. 

```{r limits-table, echo=F, message=F, warning=F, error=F}
wbe.summary.lod <- wbe %>% 
                        mutate(
                          ct.b.lod = ifelse(target=="N1", ct>qqnorm.ct.n1$lod, ct>qqnorm.ct.n2$lod),
                          ct.loq.lod = ifelse(target=="N1", 
                                              ct>qqnorm.ct.n1$loq & ct<=qqnorm.ct.n1$lod, 
                                              ct>qqnorm.ct.n2$loq & ct<=qqnorm.ct.n2$lod), 
                          ct.good = ifelse(target =="N1", 
                                           ct<=qqnorm.ct.n1$loq, 
                                           ct<=qqnorm.ct.n2$loq)
                        ) %>%
                        group_by(sample_date, facility, target, biological_replicate) %>% 
                        summarise(
                          n=n(), 
                          n.miss = sum(is.na(ct)), 
                          n.b.lod = sum(ct.b.lod, na.rm = T),
                          n.loq.lod = sum(ct.loq.lod, na.rm = T), 
                          n.good = sum(ct.good, na.rm = T)
                        ) %>% 
                        mutate_all(function(x){ifelse(is.nan(x), NA, x)}) %>% 
                        ungroup() %>%
  
                        group_by(sample_date, facility, target) %>% 
                        summarise(
                          n.bio = n(),
                          n.bio.miss = sum(n==n.miss),
                          n.bio.b.lod = sum(n==n.miss+n.b.lod), 
                          n.bio.loq.lod = sum(n==n.miss+n.b.lod+n.loq.lod),
                          n.bio.good = sum(n!=n.miss+n.b.lod+n.loq.lod),

                          n.total = sum(n), 
                          n.total.miss = sum(n.miss),
                          n.total.b.lod = sum(n.b.lod), 
                          n.total.loq.lod = sum(n.loq.lod), 
                          n.total.good = sum(n.good)
                      ) %>%
                        mutate_all(function(x){ifelse(is.nan(x), NA, x)}) %>% 
                        ungroup() %>%

                        group_by(facility, target) %>%
                        summarise(
                          n.days = n(), 
                          n.days.miss = sum(n.bio == n.bio.miss), 
                          n.days.b.lod = sum(n.bio == n.bio.b.lod), 
                          n.days.loq.lod = sum(n.bio == n.bio.loq.lod), 
                          n.days.good = sum(n.bio != n.bio.loq.lod),
                          
                          n.bio = sum(n.bio), 
                          n.bio.miss = sum(n.bio.miss), 
                          n.bio.b.lod = sum(n.bio.b.lod), 
                          n.bio.loq.lod = sum(n.bio.loq.lod), 
                          n.bio.good = sum(n.bio.good),
                          
                          n.total = sum(n.total), 
                          n.total.miss = sum(n.total.miss), 
                          n.total.b.lod = sum(n.total.b.lod), 
                          n.total.loq.lod = sum(n.total.loq.lod), 
                          n.total.good = sum(n.total.good)
                        ) %>%
                        mutate_all(function(x){ifelse(is.nan(x), NA, x)}) %>% 
                        ungroup()

wbe.lod.tr <- wbe.summary.lod %>% 
  mutate(
    total.miss = paste0(n.total.miss, " (", round(n.total.miss/n.total*100,1), ")"), 
    total.b.lod = paste0(n.total.b.lod, " (", round(n.total.b.lod/n.total*100, 2), ")"), 
    total.loq.lod = paste0(n.total.loq.lod, " (", round(n.total.loq.lod / n.total*100, 1), ")"), 
    total.good = paste0(n.total.good, " (", round(n.total.good/n.total*100,1), ")")) %>% 
  select(facility, target, total.miss, total.b.lod, total.loq.lod, total.good)




wbe.lod.bio <- wbe.summary.lod %>% 
  mutate(
    bio.miss = paste0(round(n.bio.miss/n.bio*100,1), " (", n.bio.miss, " / ", n.bio, ")"), 
    bio.b.lod = paste0(round(n.bio.b.lod/n.bio*100, 1), " (", n.bio.b.lod, " / ", n.bio, ")"), 
    bio.loq.lod = paste0(round(n.bio.loq.lod / (n.bio)*100, 1), " (", n.bio.loq.lod, " / ", n.bio, ")"), 
    bio.good = paste0(round(n.bio.good/(n.bio)*100,1), " (", n.bio.good, " / ", n.bio, ")")) %>% 
  select(facility, target, bio.miss, bio.b.lod, bio.loq.lod, bio.good)





wbe.lod.days <- wbe.summary.lod %>% 
  mutate(
    days.miss = paste0(round(n.days.miss/n.days*100,1), " (", n.days.miss, " / ", n.days, ")"), 
    days.b.lod = paste0(round(n.days.b.lod/n.days*100, 1), " (", n.days.b.lod, " / ", n.days, ")"), 
    days.loq.lod = paste0(round(n.days.loq.lod / (n.days)*100, 1), " (", n.days.loq.lod, " / ", n.days, ")"), 
    days.good = paste0(round(n.days.good/(n.days)*100,1), " (", n.days.good, " / ", n.days, ")")) %>% 
  select(facility, target, days.miss, days.b.lod, days.loq.lod, days.good)



wbe.lod.tr %<>% ungroup() %>% add_row(.before = 1) %>% add_row(.before = 1)
wbe.lod.tr[2,] <- as.list(names(wbe.lod.tr))

wbe.lod.bio %<>% ungroup() %>% add_row(.before = 1) %>% add_row(.before = 1)
wbe.lod.bio[2,] <- as.list(names(wbe.lod.bio))
names(wbe.lod.bio) <- names(wbe.lod.tr)

wbe.lod.days %<>% ungroup() %>% add_row(.before = 1) %>% add_row(.before = 1)
wbe.lod.days[2,] <- as.list(names(wbe.lod.days))
names(wbe.lod.days) <- names(wbe.lod.tr)

lod.tab <- rbind(wbe.lod.tr%>%add_row(), wbe.lod.bio%>%add_row(), wbe.lod.days)




flextable(lod.tab) %>%
  delete_part(part = "header") %>%
  border_remove() %>%
  
  border(part = "body", i = which(is.na(lod.tab$facility))[c(2,4)], border.top = fp_border_default(color = "black", width = 0.5)) %>%
  
  border(part = "body", i = c(2, which(is.na(lod.tab$facility))[c(2,4)]+2), border.bottom = fp_border_default(color = "black", width = 2)) %>% 
  
  font(part = "all", fontname = "Arial") %>%
  fontsize(part = "body", size = 8) %>%
  fontsize(part = "body", i = c(2, which(is.na(lod.tab$facility))[c(2,4)]+2), size = 9) %>%
  bold(part = "body", i = c(2, which(is.na(lod.tab$facility))[c(2,4)]+2)) %>% 
  
  merge_v(j = 1) %>% 
  
  width(j=1:2, width = 1) %>%
  width(j=3:6, width = 1.25) %>%
  
  align(part = "body", j = 3:6, align = "right") %>%
  align(part = "body", i = c(2, which(is.na(lod.tab$facility))[c(2,4)]+2), align = "center") %>%
  valign(part = "body", j = 1, valign = "top") %>% 
  
  set_caption(caption = "Distributions of Cycle Thresholds at Each Hierarchy") %>% 
  
  compose(part="body", j=1, i=c(2, which(is.na(lod.tab$facility))[c(2,4)]+2), value = as_paragraph("Wastewater Reclamation Facility")) %>% 
  compose(part="body", j=2, i=c(2, which(is.na(lod.tab$facility))[c(2,4)]+2), value = as_paragraph("Viral Sequence Target")) %>%
  compose(part="body", j=3, i=2, value = as_paragraph("Tech: Undetermined")) %>%
  compose(part="body", j=4, i=2, value = as_paragraph("Tech: Above LOD")) %>%
  compose(part="body", j=5, i=2, value = as_paragraph("Tech: Below LOD & Above LOQ")) %>%
  compose(part="body", j=6, i=2, value = as_paragraph("Tech: Below LOQ")) %>%
  
  compose(part="body", j=3, i=which(is.na(lod.tab$facility))[2]+2, value = as_paragraph("Bio: All TR Undetermined")) %>%
  compose(part="body", j=4, i=which(is.na(lod.tab$facility))[2]+2, value = as_paragraph("Bio: All TR Above LOD")) %>%
  compose(part="body", j=5, i=which(is.na(lod.tab$facility))[2]+2, value = as_paragraph("Bio: All TR Above LOQ")) %>%
  compose(part="body", j=6, i=which(is.na(lod.tab$facility))[2]+2, value = as_paragraph("Bio: Others")) %>%
  
  compose(part="body", j=3, i=which(is.na(lod.tab$facility))[5]+1, value = as_paragraph("Days: All BR Undetermined")) %>%
  compose(part="body", j=4, i=which(is.na(lod.tab$facility))[5]+1, value = as_paragraph("Days: All BR Above LOD")) %>%
  compose(part="body", j=5, i=which(is.na(lod.tab$facility))[5]+1, value = as_paragraph("Days: All BR Above LOQ")) %>%
  compose(part="body", j=6, i=which(is.na(lod.tab$facility))[5]+1, value = as_paragraph("Days: Others")) %>%
  
  compose(part="body", j=1, i=1, value = as_paragraph("A")) %>%
  compose(part="body", j=1, i=which(is.na(lod.tab$facility))[2], value = as_paragraph("B")) %>%
  compose(part="body", j=1, i=which(is.na(lod.tab$facility))[4], value = as_paragraph("C")) %>%
  bold(part="body", j=1, i=c(1, which(is.na(lod.tab$facility))[c(2,4)])) %>%
  fontsize(part="body", j=1, i=c(1, which(is.na(lod.tab$facility))[c(2,4)]), size = 12)



```

Table \@ref(tab:limits-table) has some overlap in presented data with Table \@ref(tab:prop-detect); specifically, the frequencies for undetermined results. However, the additional columns show other breakdowns of the cycle threshold values within the data.   
  
There are only `r sum(wbe.summary.lod$n.total.b.lod)` technical replicates with Ct values that are above the newly specified limit of detection (Part A, Table \@ref(tab:limits-table)).[^limit-interpret-ct] As such, the columns `Bio: All TR Above LOD` (Part B) and `Days: All BR Above LOD` (Part C) are nearly identical to their respective columns for undetermined results. 

The Part A column for `Tech: Below LOD & Above LOQ` shows the frequency of ct values falling between the limits of detection and quantification, a gray area with respect to distinguishing concentrations of genetic material. The Part A column for `Tech: Below LOQ` shows the frequency of ct values falling below the limit of quantification, a favorable range. The similarly names columns within Parts B and C have a bit different approach in their frequency calculations and, consequently, their interpretations. For Part B `Bio: All TR Above LOQ`, the frequencies refer to biological replicates where ***all*** technical replicates ct values fell above the limit of detection. The `Bio: Non-Miss Below LOQ` column gives the frequencies of biological replicates that have at least one technical replicate with a ct value below the limit of quantification. The Part C columns similarly give aggregations of biological replicates instead of technical replicates as in Part B. Note that in Parts B and C, the columns from left to right, or from all undetermined to all above LOQ correspond to increasingly inclusive thresholds so that the frequencies for all above LOQ are always greater or equal to the other two columns. Also, for Parts B and C the frequencies in the All above LOQ and Others columns sum to account for the total number of replicates or days. 

[^limit-interpret-ct]:Normally, values *below* the limit of detection would be of undetectable. However, since we are referring to the cycle thresholds, the values *above* the limit of detection are of interest. These higher Ct values correspond to lower concentrations of substrate or genetic material within the sample. 

So, after identifying these potential limitation boundaries within the data, we have a value that may be useful in managing the "missingness" / undetermined results: the limit of detection. 
  
  
  
  
# Waste (Data) Management  
  
  
## Standard Curves and Reaction Efficiency

Output of RT-qPCR is the cycle threshold (Ct) or the number of amplification cycles at which fluorescence intensity is detectable. This value in its own is not directly related to the absolute concentration / quantity of genetic material within the original sample; rather, it is a semi-quantitative value that corresponds to the relative concentrations following serial amplification. In order to convert these cycle threshold values to estimates of the absolute concentration of genetic material within the original sample, we need to calibrate conversion values to represent the implemented RT-qPCR reaction.


### Algebraic Foundations of Amplification
  
As an aside (and more to walk myself through the understanding), the following equations from Kralik and Ricchi (2017) are explored. 


\begin{equation}
  N_n = N_0 \times (1+E)^n
  (\#eq:doubling)
\end{equation}


Equation \@ref(eq:doubling) shows the equation linking an original sample concentration ($N_0$) to the concentration after *n* round of amplification ($N_n$). The parameter *E* corresponds to the efficiency of the reaction: for a perfectly efficient reaction, $E=1$ and the term $(1+E)^n = 2^n$ reflects the doubling of genetic material for each of *n* rounds of amplification (Equation \@ref(eq:doubling-2)). 

\begin{equation}
  N_n/N_0 = 2^n
  (\#eq:doubling-2)
\end{equation}

Typically, the samples with known concentrations in ten-fold serial dilutions are used to estimate calibration or standard curves for the reactions; Equation \@ref(eq:ten-fold) represents a simplification of Equation \@ref(eq:doubling-2) using two ten-fold dilutions.

\begin{equation}
  10 = 2^n
  (\#eq:ten-fold)
\end{equation}

Now, solving Equation \@ref(eq:ten-fold) yields Equation \@ref(eq:ten-fold-solved) showing that under a perfectly efficient doubling reaction, the difference in the number of amplification rounds between any two ten-fold dilutions should approximate 3.322. 

\begin{equation}
  log_2(10) = n \approx 3.322
  (\#eq:ten-fold-solved)
\end{equation}

This known value can thusly be used to calculate the reaction's efficiency *E* as shown in Equation \@ref(eq:efficiency). 

\begin{equation}
  E = 10^{-(\frac{1}{n})}-1
  (\#eq:efficiency)
\end{equation}

Altogether, this framework can be used to convert RT-qPCR output Ct values to an absolute concentration of genetic material. 



### Fitting Standard Curves

The standard curves are fit to the data using linear regression of cycle threshold on base 10 logarithm of the known quantity of substrate (Equation \@ref(eq:sc-fit)). The standard curve data was generated using many experimental repetitions (denoted with CN=collection number). Table \@ref(tab:standard-curves) shows some output of the linear regressions on these standard curve data. 

\begin{equation}
  Ct = \beta_0 + \beta_1 log_{10}(quantity) + \epsilon
  (\#eq:sc-fit)
\end{equation}


```{r standard-curves, echo = F, message=F, warning=F, error=F}
# summary(qc)
# plot(ct~log10(quantity), data=qc[which(qc$target=="N1" & qc$df=="qc1"),])
# lm(ct~log10(quantity), data=qc[which(qc$target=="N1" & qc$df=="qc1"),])
# 
# plot(ct~log10(quantity), data=qc[which(qc$target=="N2" & qc$df=="qc1"),])
# lm(ct~log10(quantity), data=qc[which(qc$target=="N2" & qc$df=="qc1"),])

qc.means <- qc %>% filter(df=="qc1") %>% group_by(target, quantity) %>% summarise(n= n(), mean.ct = mean(ct), sd.ct = sd(ct))

sc.mean.n1 <- lm(mean.ct~log10(quantity), data=qc.means[which(qc.means$target=="N1"),])
sc.mean.n2 <- lm(mean.ct~log10(quantity), data=qc.means[which(qc.means$target=="N2"),])

# i=1
# repeat{
#   cn <- unique(qc$collection_num)[i]
#   
#   cat("\n \n \n")
#   print(cn)
#   
#   cat("\n \n \n N1")
#   try(print(summary(lm(ct~log10(quantity), data=qc[which(qc$target=="N1" & qc$collection_num==cn), ]))))
#   
#   cat("\n \n \n N2")
#   try(print(summary(lm(ct~log10(quantity), data=qc[which(qc$target=="N2" & qc$collection_num==cn), ]))))
#   
#   continue <- readline("Continue?")
#   
#   if(continue==0){break}
#   i = i+1
# }

n1.cn <- 13
n2.cn <- 24



qc.by.target <- split(qc %>% filter(df=="qc1"), f = qc$target[which(qc$df=="qc1")])

qc.by.target.by.collection.number <- lapply(qc.by.target, function(x){split(x, f = x$collection_num)})

qc.standard.curves <- lapply(qc.by.target.by.collection.number, function(x){lapply(x, function(y){lm(ct~log10(quantity), data=y)})})

qc.sc.ests <- lapply(qc.standard.curves, function(x){lapply(x, function(y){c(coef(y), r2=summary(y)$r.squared)})})

qc.sc.ests <- lapply(qc.sc.ests, function(x){bind_rows(x, .id = "CN")}) %>% bind_rows(.id="target")

qc.sc.ests$Efficiency <- 10^(-1/qc.sc.ests$`log10(quantity)`)-1

qc.sc.ests.summary <- qc.sc.ests %>% 
  group_by(target) %>% 
  summarise(
    `(Intercept)`=paste0(round(mean(`(Intercept)`), 3), " (", round(sd(`(Intercept)`),3), ")"), 
    `log10(quantity)`=paste0(round(mean(`log10(quantity)`),3), " (", round(sd(`log10(quantity)`),3), ")"), 
    r2 = paste0(round(mean(r2),3), " (", round(sd(r2),3), ")"), 
    Efficiency = paste0(round(mean(Efficiency),3), " (", round(sd(Efficiency),3), ")")
    ) %>% 
  ungroup() %>% 
  mutate(CN = "MEAN (SD)") %>% 
  bind_rows(qc.sc.ests%>%mutate_if(is.numeric, ~round(.,3))%>%mutate_all(~as.character(.)), .) %>% 
  arrange(target, CN)





as_grouped_data(qc.sc.ests.summary, "target") %>% 
  flextable() %>% 
  border_remove() %>% 
  align(part = "all", align = "right") %>%
  align(part = "header", align = "center") %>%
  align(part = "body", j = 1, align = "left") %>%
  hline(part = "header", i = 1, border = fp_border_default(color = "black", width = 2)) %>%
  hline(part = "body", i = which(!is.na(as_grouped_data(qc.sc.ests.summary, "target")$target)), border = fp_border_default(color = "black", width = 0.5)) %>% 
  bold(part = "header") %>% 
  bold(part = "body", i = which({qc.sc.ests.summary$CN==13 & qc.sc.ests.summary$target=="N1"} | {qc.sc.ests.summary$CN==24 & qc.sc.ests.summary$target=="N2"}) + c(1,2)) %>%
  font(part = "all", fontname = "Arial") %>%
  fontsize(part = "header", size = 9) %>% 
  fontsize(part = "body", size = 8)%>%
  autofit() %>% 
  set_caption("Standard Curve Linear Regression Fits")
 

```






The rows for target N1, collection number 13 and target N2, collection number 24 are bold as they were selected as the ideal candidates for the calibration. Briefly, these single runs were chosen due to the variability in the standard curves as a whole. The variability in the data is not representative of the process itself, rather due mainly to the quality of the samples used in the experiment; the samples with known concentrations of substrate are known to degrade over time, yielding inconsistent results.

Figure \@ref(fig:sc-plots) shows fit lines for each repetition of the standard curve calibration experiments, highlighting the mean of all fits (grey) and the single results chosen to represent the reaction (black line for fit line, black circles for data points). 



```{r sc-plots, echo=F, message=F, warning=F, error=F, fig.height=6*9/16*2, fig.cap = "RT-qPCR Standard Curves for Viral Sequence Targets, N1 & N2"}


par(mfcol = c(2,1), mar = c(2.1, 2.1, 1.1, 0))


plot(0,type='n',axes=F, ylim = c(20,40), xlim = c(0,5), xlab = "", ylab = "", main = "")
axis(1, at = 0:5, tick = T, labels = T, cex.axis = 0.7, line = 0, padj = -1.75, tck = -0.0125)
title(xlab = "log10(quantity)", line = 1)

axis(2, at = seq(20,40,by=5), tick = T, labels = T, cex.axis = 0.7, line = 0, hadj = 0.25, tck = -0.0125, las = 1)
title(ylab = "Cycle Threshold", line = 1.25)


abline(sc.mean.n1, lwd = 10, col = "darkgrey")


for(i in 1:length(qc.standard.curves$N1)){
  abline(qc.standard.curves$N1[[i]], lty = 3)
}
abline(qc.standard.curves$N1$`13`, lwd = 4)
points(qc.by.target.by.collection.number$N1$`13`$ct~log10(qc.by.target.by.collection.number$N1$`13`$quantity), cex = 2)
text(par('usr')[2],par('usr')[4],labels = paste("13: Ct =",round(coef(qc.standard.curves$N1$`13`)[1],3), round(coef(qc.standard.curves$N1$`13`)[2],3), "*log10(quantity)"), adj = c(1, 1.2))
text(par('usr')[2],par('usr')[4],labels = paste("Mean: Ct =",round(coef(sc.mean.n1)[1],3), round(coef(sc.mean.n1)[2],3), "*log10(quantity)"), adj = c(1, 2.4))

# title(main = "Standard Curves for N1")
box()
text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "A", adj = c(0,1), xpd = T, cex = 1, font = 2)




plot(0,type='n',axes=F, ylim = c(20,40), xlim = c(0,5), xlab = "", ylab = "", main = "")
axis(1, at = 0:5, tick = T, labels = T, cex.axis = 0.7, line = 0, padj = -1.75, tck = -0.0125)
title(xlab = "log10(quantity)", line = 1)

axis(2, at = seq(20,40, by=5), tick = T, labels = T, cex.axis = 0.7, line = 0, hadj = 0.25, tck = -0.0125, las = 1)
title(ylab = "Cyle Threshold", line = 1.25)




abline(sc.mean.n2, lwd = 10, col = "darkgrey")

for(i in 1:length(qc.standard.curves$N2)){
  abline(qc.standard.curves$N2[[i]], lty = 3)
}
abline(qc.standard.curves$N2$`24`, lwd = 4)
points(qc.by.target.by.collection.number$N2$`24`$ct~log10(qc.by.target.by.collection.number$N2$`24`$quantity), cex = 2)
text(par('usr')[2],par('usr')[4],labels = paste("24: Ct =",round(coef(qc.standard.curves$N2$`24`)[1],3), round(coef(qc.standard.curves$N2$`24`)[2],3), "*log10(quantity)"), adj = c(1, 1.2))
text(par('usr')[2],par('usr')[4],labels = paste("Mean: Ct =",round(coef(sc.mean.n2)[1],3), round(coef(sc.mean.n2)[2],3), "*log10(quantity)"), adj = c(1, 2.4))
# title(main = "Standard Curves for N2")
box()
text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "B", adj = c(0,1), xpd = T, cex = 1, font = 2)


```


### Calculating Sample Concentrations using Calibrated Standard Curves and Reaction Dilutions

Now with the fit lines to the standard curve data, we can use the values from the intercepts and the slopes to convert cycle threshold values to the original sample concentration (Equation , a rearrangement of Equation \@ref(eq:sc-fit)). 

\begin{equation}
  Quantity = 10^{-\frac{Ct - \beta_0}{\beta_1}}
  (\#eq:ct-conversion)
\end{equation}


Substituting our fit estimates for the intercept and slops we have Equation \@ref(eq:ct-seqs).


\begin{equation}
  \begin{split}
      &Quantity_{N1} = 10^{-\frac{Ct - 34.008}{-3.3890}} \\
      &Quantity_{N2} = 10^{-\frac{Ct - 32.416}{-3.3084}}
  \end{split}
  (\#eq:ct-seqs)
\end{equation}

The concentrations yielded from Equation \@ref(eq:ct-seqs) would be of the unit $copy~ per ~\mu L~of~reaction$. We need to take into consideration the dilution scheme (Equation \@ref(eq:dilution)) to have the unit be simply $copy~per~\mu L$. 




## Data Adjustment using Limits of Detection and Quantification

It is relatively common to use the limit of detection to replace undetermined results within the data. So, I have concocted a few scenarios and created a few datasets varying on the replacement of undetermined values within the dataset. Briefly, scenario 1 uses the data as is without replacing undetermined values. Scenario 2 uses half of the limit of detection to replace undetermined values and half the limit of quantification to replace the data with concentrations below that limit. Scenario 3 again uses half of the limit of detection for undetermined, but also scales values between the LOD and LOQ to stretch from the LOQ to the LOD/2. Scenario 4 replaces undetermined values with the LOD scaled to incorporate the relative frequency of undetermined results and, then, scales the values between the LOD and LOQ to the missing frequency-scaled LOD. Scenario 5 does not replace undetermined values, but does scale the values below the LOQ to half of the LOD. Figure \@ref(fig:scenario-hists) shows histograms of each of the datasets. 


```{r, echo = F, message=F, warning=F, error=F}
n1.int <- qc.sc.ests$`(Intercept)`[which(qc.sc.ests$target=="N1" & qc.sc.ests$CN==n1.cn)]
n1.slope <- qc.sc.ests$`log10(quantity)`[which(qc.sc.ests$target=="N1" & qc.sc.ests$CN==n1.cn)]
n2.int <- qc.sc.ests$`(Intercept)`[which(qc.sc.ests$target=="N2" & qc.sc.ests$CN==n2.cn)]
n2.slope <- qc.sc.ests$`log10(quantity)`[which(qc.sc.ests$target=="N2" & qc.sc.ests$CN==n2.cn)]

wbe %<>% 
  mutate(copies_per_uL_rxn = ifelse(target=="N1", 
                                           10^(-(n1.int-ct)/n1.slope), 
                                           10^(-(n2.int-ct)/n2.slope)
                                           )
                ) %>%
  mutate(copies_per_uL = copies_per_uL_rxn*20/2*25/3*60)      

n1.lod.copies_per_uL <- 10^(-(n1.int-qqnorm.ct.n1$lod)/n1.slope) * 20/2*25/3*60
n1.loq.copies_per_uL <- 10^(-(n1.int-qqnorm.ct.n1$loq)/n1.slope) * 20/2*25/3*60
n2.lod.copies_per_uL <- 10^(-(n2.int-qqnorm.ct.n2$lod)/n2.slope) * 20/2*25/3*60
n2.loq.copies_per_uL <- 10^(-(n2.int-qqnorm.ct.n2$loq)/n2.slope) * 20/2*25/3*60



# Scenario 1 -- raw data
## leave missings out, calculate correlations with positive results only
## leave all values between LOD and LOQ as is







# Scenario 2 -- simple imputation
## use LOD/2 to impute missings
## use LOQ/2 to adjust values between LOD and LOQ


wbe2 <- wbe %>% 
          mutate(copies_per_uL = ifelse(target == "N1", 
                                        ifelse(
                                          copies_per_uL < n1.lod.copies_per_uL | is.na(copies_per_uL), 
                                          n1.lod.copies_per_uL/2, 
                                          ifelse(
                                            copies_per_uL < n1.loq.copies_per_uL, 
                                            n1.loq.copies_per_uL/2,
                                            copies_per_uL
                                          )),
                                        ifelse(
                                          copies_per_uL < n2.lod.copies_per_uL | is.na(copies_per_uL), 
                                          n2.lod.copies_per_uL/2,
                                          ifelse(
                                            copies_per_uL < n2.loq.copies_per_uL, 
                                            n2.loq.copies_per_uL/2,
                                            copies_per_uL
                                          )
                                        )
                                  )
                 )









# Scenario 3 -- simple imputation with scaled LOQ
## use LOD/2 to impute missings
## scale values between LOD and LOQ from LOQ to LOD/2

n1.lod.loq.min <- min(wbe$copies_per_uL[which(wbe$copies_per_uL>=n1.lod.copies_per_uL & wbe$copies_per_uL<n1.loq.copies_per_uL & wbe$target=="N1")], na.rm = T)

n1.lod.loq.max <- max(wbe$copies_per_uL[which(wbe$copies_per_uL>=n1.lod.copies_per_uL & wbe$copies_per_uL<n1.loq.copies_per_uL & wbe$target=="N1")], na.rm = T)

n1.lod.loq.rdiff <- diff(range(wbe$copies_per_uL[which(wbe$copies_per_uL>=n1.lod.copies_per_uL & wbe$copies_per_uL<n1.loq.copies_per_uL & wbe$target=="N1")], na.rm = T))


n2.lod.loq.min <- min(wbe$copies_per_uL[which(wbe$copies_per_uL>=n2.lod.copies_per_uL & wbe$copies_per_uL<n2.loq.copies_per_uL & wbe$target=="N2")], na.rm = T)

n2.lod.loq.max <- max(wbe$copies_per_uL[which(wbe$copies_per_uL>=n2.lod.copies_per_uL & wbe$copies_per_uL<n2.loq.copies_per_uL & wbe$target=="N2")], na.rm = T)

n2.lod.loq.rdiff <- diff(range(wbe$copies_per_uL[which(wbe$copies_per_uL>=n2.lod.copies_per_uL & wbe$copies_per_uL<n2.loq.copies_per_uL & wbe$target=="N2")], na.rm = T))




wbe3 <- wbe %>% 
          mutate(copies_per_uL = ifelse(target == "N1", 
                                        ifelse(
                                          copies_per_uL < n1.lod.copies_per_uL | is.na(copies_per_uL), 
                                          n1.lod.copies_per_uL/2, 
                                          ifelse(
                                            copies_per_uL < n1.loq.copies_per_uL, 
                                            copies_per_uL+(copies_per_uL-n1.lod.loq.max)/n1.lod.loq.rdiff*(n1.lod.loq.min-(n1.lod.copies_per_uL/2)),
                                            copies_per_uL
                                          )),
                                        ifelse(
                                          copies_per_uL < n2.lod.copies_per_uL | is.na(copies_per_uL), 
                                          n2.lod.copies_per_uL/2,
                                          ifelse(
                                            copies_per_uL < n2.loq.copies_per_uL, 
                                            copies_per_uL+(copies_per_uL-n2.lod.loq.max)/n2.lod.loq.rdiff*(n2.lod.loq.min-(n2.lod.copies_per_uL/2)),
                                            copies_per_uL
                                          )
                                        )
                                  )
                 )

# View(cbind(wbe$copies_per_uL[which(wbe$copies_per_uL>=n1.lod.copies_per_uL & wbe$copies_per_uL<n1.loq.copies_per_uL & wbe$target=="N1")], wbe3$copies_per_uL[which(wbe$copies_per_uL>=n1.lod.copies_per_uL & wbe$copies_per_uL<n1.loq.copies_per_uL & wbe$target=="N1")]))
# 
# 
# View(cbind(wbe$copies_per_uL[which(wbe$copies_per_uL>=n2.lod.copies_per_uL & wbe$copies_per_uL<n2.loq.copies_per_uL & wbe$target=="N2")], wbe3$copies_per_uL[which(wbe$copies_per_uL>=n2.lod.copies_per_uL & wbe$copies_per_uL<n2.loq.copies_per_uL & wbe$target=="N2")]))






# Scenario 4 -- scaling imputation
## use LOD scaled by proportion of missings to impute missings
### if 1/2 tech reps are missing, then LOD*0.5
## scale values between LOD and LOQ from LOQ to scaled LOD



wbe4 <- wbe %>% left_join(., wbe.summary.br %>% mutate(p.miss.tr = n.total.miss / n.total) %>% select(sample_date, facility, target, p.miss.tr), by = c("sample_date", "facility", "target")) %>% 
          mutate(copies_per_uL = ifelse(target == "N1", 
                                        ifelse(
                                          copies_per_uL < n1.lod.copies_per_uL | is.na(copies_per_uL), 
                                          n1.lod.copies_per_uL/(2+2*p.miss.tr), 
                                          ifelse(
                                            copies_per_uL < n1.loq.copies_per_uL, 
                                            copies_per_uL+(copies_per_uL-n1.lod.loq.max)/n1.lod.loq.rdiff*(n1.lod.loq.min-(n1.lod.copies_per_uL/(2+2*p.miss.tr))),
                                            copies_per_uL
                                          )),
                                        ifelse(
                                          copies_per_uL < n2.lod.copies_per_uL | is.na(copies_per_uL), 
                                          n2.lod.copies_per_uL/(2+2*p.miss.tr),
                                          ifelse(
                                            copies_per_uL < n2.loq.copies_per_uL, 
                                            copies_per_uL+(copies_per_uL-n2.lod.loq.max)/n2.lod.loq.rdiff*(n2.lod.loq.min-(n2.lod.copies_per_uL/(2+2*p.miss.tr))),
                                            copies_per_uL
                                          )
                                        )
                                  )
                 )


# View(cbind(wbe$copies_per_uL[which(wbe$copies_per_uL>=n1.lod.copies_per_uL & wbe$copies_per_uL<n1.loq.copies_per_uL & wbe$target=="N1")], wbe4$copies_per_uL[which(wbe$copies_per_uL>=n1.lod.copies_per_uL & wbe$copies_per_uL<n1.loq.copies_per_uL & wbe$target=="N1")]))
# 
# 
# View(cbind(wbe$copies_per_uL[which(wbe$copies_per_uL>=n2.lod.copies_per_uL & wbe$copies_per_uL<n2.loq.copies_per_uL & wbe$target=="N2")], wbe4$copies_per_uL[which(wbe$copies_per_uL>=n2.lod.copies_per_uL & wbe$copies_per_uL<n2.loq.copies_per_uL & wbe$target=="N2")]))



# Scenario 5
## leave missings, scale loq


wbe5 <- wbe %>% 
          mutate(copies_per_uL = ifelse(target == "N1", 
                                        ifelse(
                                          copies_per_uL < n1.lod.copies_per_uL | is.na(copies_per_uL), 
                                          NA, 
                                          ifelse(
                                            copies_per_uL < n1.loq.copies_per_uL, 
                                            copies_per_uL+(copies_per_uL-n1.lod.loq.max)/n1.lod.loq.rdiff*(n1.lod.loq.min-(n1.lod.copies_per_uL/2)),
                                            copies_per_uL
                                          )),
                                        ifelse(
                                          copies_per_uL < n2.lod.copies_per_uL | is.na(copies_per_uL), 
                                          NA,
                                          ifelse(
                                            copies_per_uL < n2.loq.copies_per_uL, 
                                            copies_per_uL+(copies_per_uL-n2.lod.loq.max)/n2.lod.loq.rdiff*(n2.lod.loq.min-(n2.lod.copies_per_uL/2)),
                                            copies_per_uL
                                          )
                                        )
                                  )
                 )
```


```{r scenario-hists, echo = F, message=F, warning=F, error=F, fig.height=6*9/16*2, fig.cap = "Histograms of the Natural Logarithm of Sample Viral Sequence Copy Concentrations"}

par(mfrow = c(3,2), mar = c(2.1,2.1,0,0))

hist(log(wbe$copies_per_uL), breaks = 100, axes = F, xlab = "", ylab = "", main = "", xlim = c(4,10))
abline(v=log(n1.lod.copies_per_uL), lwd = 1)
abline(v=log(n1.loq.copies_per_uL), lwd = 0.5)
abline(v=log(n2.lod.copies_per_uL), lwd = 1, lty = 3)
abline(v=log(n2.loq.copies_per_uL), lwd = 0.5, lty = 3)
axis(1, at = 4:10, labels = T, tick = T, cex.axis = 0.7, line = 0, padj = -1.75, tck = -0.0125)
title(xlab = expression(paste("log(Copies per ", mu, "L)")), line = 1)
axis(2, at = seq(0,40,by=10), tick = T, labels = T, cex.axis = 0.7, line = 0, hadj = 0.25, tck = -0.0125, las = 1)
title(ylab = "Frequency", line = 1.25)
# legend("topright", lwd = c(2,1,2,1), lty = c(1,1,3,3), legend = c(expression(LOD[~N1]), expression(LOQ[~N1]), expression(LOD[~N2]),expression(LOQ[~N2])))
text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "A", adj = c(0,1), xpd = T, cex = 1, font = 2)


hist(log(wbe2$copies_per_uL), breaks = 100, axes = F, xlab = "", ylab = "", main = "", xlim = c(4,10))
abline(v=log(n1.lod.copies_per_uL), lwd = 1)
abline(v=log(n1.loq.copies_per_uL), lwd = 0.5)
abline(v=log(n2.lod.copies_per_uL), lwd = 1, lty = 3)
abline(v=log(n2.loq.copies_per_uL), lwd = 0.5, lty = 3)
axis(1, at = 4:10, labels = T, tick = T, cex.axis = 0.7, line = 0, padj = -1.75, tck = -0.0125)
title(xlab = expression(paste("log(Copies per ", mu, "L)")), line = 1)
axis(2, at = seq(0,800,by=100), tick = T, labels = T, cex.axis = 0.7, line = 0, hadj = 0.25, tck = -0.0125, las = 1)
title(ylab = "Frequency", line = 1.25)
# legend("topright", lwd = c(2,1,2,1), lty = c(1,1,3,3), legend = c(expression(LOD[~N1]), expression(LOQ[~N1]), expression(LOD[~N2]),expression(LOQ[~N2])))
text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "B", adj = c(0,1), xpd = T, cex = 1, font = 2)


hist(log(wbe3$copies_per_uL), breaks = 100, axes = F, xlab = "", ylab = "", main = "", xlim = c(4,10))
abline(v=log(n1.lod.copies_per_uL), lwd = 1)
abline(v=log(n1.loq.copies_per_uL), lwd = 0.5)
abline(v=log(n2.lod.copies_per_uL), lwd = 1, lty = 3)
abline(v=log(n2.loq.copies_per_uL), lwd = 0.5, lty = 3)
axis(1, at = 4:10, labels = T, tick = T, cex.axis = 0.7, line = 0, padj = -1.75, tck = -0.0125)
title(xlab = expression(paste("log(Copies per ", mu, "L)")), line = 1)
axis(2, at = seq(0,800,by=100), tick = T, labels = T, cex.axis = 0.7, line = 0, hadj = 0.25, tck = -0.0125, las = 1)
title(ylab = "Frequency", line = 1.25)
# legend("topright", lwd = c(2,1,2,1), lty = c(1,1,3,3), legend = c(expression(LOD[~N1]), expression(LOQ[~N1]), expression(LOD[~N2]),expression(LOQ[~N2])))
text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "C", adj = c(0,1), xpd = T, cex = 1, font = 2)


hist(log(wbe4$copies_per_uL), breaks = 100, axes = F, xlab = "", ylab = "", main = "", xlim = c(4,10))
abline(v=log(n1.lod.copies_per_uL), lwd = 1)
abline(v=log(n1.loq.copies_per_uL), lwd = 0.5)
abline(v=log(n2.lod.copies_per_uL), lwd = 1, lty = 3)
abline(v=log(n2.loq.copies_per_uL), lwd = 0.5, lty = 3)
axis(1, at = 4:10, labels = T, tick = T, cex.axis = 0.7, line = 0, padj = -1.75, tck = -0.0125)
title(xlab = expression(paste("log(Copies per ", mu, "L)")), line = 1)
axis(2, at = seq(0,200,by=50), tick = T, labels = T, cex.axis = 0.7, line = 0, hadj = 0.25, tck = -0.0125, las = 1)
title(ylab = "Frequency", line = 1.25)
# legend("topright", lwd = c(2,1,2,1), lty = c(1,1,3,3), legend = c(expression(LOD[~N1]), expression(LOQ[~N1]), expression(LOD[~N2]),expression(LOQ[~N2])))
text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "D", adj = c(0,1), xpd = T, cex = 1, font = 2)



hist(log(wbe5$copies_per_uL), breaks = 100, axes = F, xlab = "", ylab = "", main = "", xlim = c(4,10))
abline(v=log(n1.lod.copies_per_uL), lwd = 1)
abline(v=log(n1.loq.copies_per_uL), lwd = 0.5)
abline(v=log(n2.lod.copies_per_uL), lwd = 1, lty = 3)
abline(v=log(n2.loq.copies_per_uL), lwd = 0.5, lty = 3)
axis(1, at = 4:10, labels = T, tick = T, cex.axis = 0.7, line = 0, padj = -1.75, tck = -0.0125)
title(xlab = expression(paste("log(Copies per ", mu, "L)")), line = 1)
axis(2, at = seq(0,20,by=5), tick = T, labels = T, cex.axis = 0.7, line = 0, hadj = 0.25, tck = -0.0125, las = 1)
title(ylab = "Frequency", line = 1.25)
# legend("topright", lwd = c(2,1,2,1), lty = c(1,1,3,3), legend = c(expression(LOD[~N1]), expression(LOQ[~N1]), expression(LOD[~N2]),expression(LOQ[~N2])))
text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "E", adj = c(0,1), xpd = T, cex = 1, font = 2)

plot.new()
legend(x=0, y=1, lwd = c(1,0.5,1,0.5), lty = c(1,1,3,3), legend = c(expression(LOD[~N1]), expression(LOQ[~N1]), expression(LOD[~N2]),expression(LOQ[~N2])), cex = 2, xjust = 0, yjust = 1)

text(x = 0.75, y=1, labels = "A: Scenario 1\nB: Scenario 2\nC: Scenario 3\nD: Scenario 4\nE: Scenario 5", adj = c(0, 1), cex = 1)

```






# Wiping away Convolution

There are certain inherent flaws within most, if not all, infectious disease surveillance systems. Perhaps most well known is the discrepancy in the times a case is reported through the surveillance system and the times that person is potentially infectious and transmitting. These discrepancies essentially leave public health professionals with an imperfect understanding of the true epidemic curve. 

The data available for COVID-19 cases for Georgia attempt to overcome this issue. In addition to the more widely available time series of cases by date of report, the Georgia Department of Public Health has also included time series of cases by the date of symptom onset. This subtle difference has a meaningful rationale as it would better demarcate the times at which the cases were potentially infectious and contributing to transmission. Similarly, this would give us a better glimpse at the times at which cases were shedding into their environment. However, there is a bit of a limitation still within the times series of cases by symptom onset. The data documentation indicates that the dates correspond to the date of symptom onset only when that information is available and, otherwise, refer to dates of positive specimen collection. A date of positive specimen collection may still be preferable to report date, but it is likely there still exists some discrepancy with that of the true transmissible period.

Figure \@ref(fig:epi-curve) shows the epidemic curve for Athens-Clarke County with both cases by dates of report and symptom onset. The difference in the two curves is slight, but a "left" transition is notable. 


```{r epi-curve, echo = F, message=F, warning=F, error=F, fig.height=6*9/16*2, fig.cap = "Epidemic Curve of COVID-19 in Athens-Clarke County, GA, USA"}

date.labels <- seq(min(covid$date), max(covid$date), by="months")

at.points <- seq(0.5, nrow(covid)-0.5, by=1)[which(covid$date%in%date.labels)]

at.points2 <- seq(0.5, nrow(covid)-0.5, by=1)[which(covid$date%in%c(min(wbe$sample_date), max(wbe$sample_date)))]

layout(matrix(c(1,2,1,3), nrow = 2), widths = c(1,1), heights = c(3,1))

par(mar=c(2.1,3.1,2.1,1.1))
# barplot(cases.reported~date, data=covid)
barplot(cases.reported~date, data=covid, las=2, xlab = '', ylab = '', xaxt = 'n', yaxt = 'n', width = 1, space = 0, xaxs = 'i', yaxs = 'i', axes=F, col = "gainsboro", border = "gainsboro")
axis(1, at=at.points, tick=T, labels = paste(format(date.labels, "%b %y"),' '), xpd = TRUE, cex.axis=0.7, line = 0, padj = -1)
axis(2, labels = T, tick = T, cex=0.7, cex.axis = 0.7, las = 1)
title(ylab = "Cases", line = 2)
axis(3, at = at.points2, labels = format(c(min(wbe$sample_date), max(wbe$sample_date)), "%d %b %y"), tick = T, line = 0, tck = 0.02, cex.axis = 0.7)
axis(3, at = mean(at.points2), labels = "Study Period", line = 0, padj = -0.25, tck = -0.04)


barplot(cases.symptom.onset~date, data=covid, las=2, xlab = '', ylab = '', xaxt = 'n', yaxt = 'n', width = 1, space = 0, xaxs = 'i', yaxs = 'i', axes=F, add = T, col = viridis::viridis(100, alpha = 0.75)[55], border = viridis::viridis(100, alpha = 0.75)[55])

legend("topleft", fill = c("gainsboro", viridis::viridis(100, alpha = 0.75)[55]), legend = c("Reported", "Symptom Onset"))
text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "A", adj = c(0,1), xpd = T, cex = 1, font = 2)


par(mar = c(2.1, 1.1, 1.1, 1.1))
barplot(cases.reported~date, data=covid, las=2, xlab = '', ylab = '', xaxt = 'n', yaxt = 'n', width = 1, space = 0, xaxs = 'i', yaxs = 'i', axes=F, col = "black", ylim = c(0, 300))
axis(1, at=at.points, tick=T, labels = paste(format(date.labels, "%b %y"),' '), xpd = TRUE, cex.axis=0.7, line = 0, padj = -2, tck = -0.02)
axis(2, labels = F, tick = T, cex=0.7, cex.axis = 0.7, las = 1)
title(xlab = "Date of Report", line = 0.75)
# axis(3, at = at.points2, labels = format(c(min(wbe$sample_date), max(wbe$sample_date)), "%d %b %y"), tick = T, line = 0, tck = 0.02, cex.axis = 0.7)
# axis(3, at = mean(at.points2), labels = "Study Period", line = 0, padj = -0.25, tck = -0.04)
text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "B", adj = c(0,1), xpd = T, cex = 1, font = 2)



barplot(cases.symptom.onset~date, data=covid, las=2, xlab = '', ylab = '', xaxt = 'n', yaxt = 'n', width = 1, space = 0, xaxs = 'i', yaxs = 'i', axes=F, col = viridis::viridis(100, alpha = 0.75)[55], border = viridis::viridis(100, alpha = 0.75)[55], ylim = c(0, 300))
axis(1, at=at.points, tick=T, labels = paste(format(date.labels, "%b %y"),' '), xpd = TRUE, cex.axis=0.7, line = 0, padj = -2, tck = -0.02)
axis(2, labels = F, tick = T, cex=0.7, cex.axis = 0.7, las = 1)
title(xlab = "Date of Symptom Onset", line = 0.75)
# axis(3, at = at.points2, labels = format(c(min(wbe$sample_date), max(wbe$sample_date)), "%d %b %y"), tick = T, line = 0, tck = 0.02, cex.axis = 0.7, hadj = 1)
# axis(3, at = mean(at.points2), labels = "Study Period", line = 0, padj = -0.25, tck = -0.04)
text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "C", adj = c(0,1), xpd = T, cex = 1, font = 2)


```


## Comparing Deconvoluted Case Counts by Report Date to Case Counts By Symptom Onset Date 

The case counts by report date are essentially a *convolution* of the underlying true epidemic curve. As previously mentioned, there are reporting delays associated with each case. If we have some understanding of the distribution of these delays (i.e., how likely a delay of *n* amount of time is), then we can use a method of deconvolution to attempt to remove the delays in the data. The `incidental` package uses an empirical Bayes estimation method to accomplish this. Furthermore, the package also has a given delay distribution for COVID-19 (Figure \@ref(fig:delay-dist)). 

```{r delay-dist, echo = F, message=F, warning=F, error=F, fig.cap = "Incidental Package Delay Distribution for COVID-19"}
par(mar = c(4.1, 4.1, 1.1, 1.1))
plot(incidental::covid_delay_dist$days, incidental::covid_delay_dist$case, type = "l", xlab = "Days of Delay", ylab = "Probability")
# lines(incidental::covid_delay_dist$days, incidental::covid_delay_dist$hospitalization, lty = 5)

arrows(x0 = which.max(incidental::covid_delay_dist$case), x1 = which.max(incidental::covid_delay_dist$case), y0 = par('usr')[4], y1 = incidental::covid_delay_dist$case[which.max(incidental::covid_delay_dist$case)]+0.0005, length = par('pin')[2]*0.01)


# text(x = par('usr')[2]-0.24*par('usr')[2], y = par('usr')[4]-0.2*par('usr')[4], labels = paste(c("Days", incidental::covid_delay_dist$days[which(incidental::covid_delay_dist$case!=0)]), collapse = "\n"), cex = 0.5, adj = c(0, 1))
# 
# text(x = par('usr')[2]-0.12*par('usr')[2], y = par('usr')[4]-0.2*par('usr')[4], labels = paste(c("Probability (%)", round(100*incidental::covid_delay_dist$case[which(incidental::covid_delay_dist$case!=0)], 3)), collapse = "\n"), cex = 0.5, adj = c(1, 1))
# 
# text(x = par('usr')[2], y = par('usr')[4]-0.2*par('usr')[4], labels = paste(c("Cumulative P (%)", round(100*cumsum(incidental::covid_delay_dist$case[which(incidental::covid_delay_dist$case!=0)]), 3)), collapse = "\n"), cex = 0.5, adj = c(1, 1))


lines(incidental::covid_delay_dist$days, cumsum(incidental::covid_delay_dist$case)*par('usr')[4], lwd = 2, lty = 5)

legend("topright", lty = c(1,5), lwd = c(1,2), legend = c("PMF", "CDF"))

```


The estimation procedure for the deconvoluted incidence curve includes the fitting of splines to the convoluted data as Poisson basis functions for a regularized Poisson likelihood function. Therefore, it is necessary to include a degrees of freedom hyperparameter for the splines fits and a hyperparameter, $\lambda$, for the regularization. A vector of potential values is explored for both hyperparameters and the algorithm will select the best performing ones based on AIC for spline degress of freedom and validation likelihood for lambda. 

```{r, echo=F, message=F, warning=F, error=F}
# dof_grid_try = seq(30, 50, 1)
# lam_grid_try = 10^(seq(-0.5, -10, length.out = 30))
# 
# 
# decon.report <- incidental::fit_incidence(covid$cases.reported, 
#                                           incidental::covid_delay_dist$case, 
#                                           dof_grid = dof_grid_try, 
#                                           lam_grid = lam_grid_try) 

# save(decon.report, file = "./consult/01-data/decon_report.rds")
load("./consult/01-data/decon_report.rds")
# decon.symptom <- incidental::fit_incidence(covid$cases.symptom.onset, 
#                                            incidental::covid_delay_dist$case, 
#                                            dof_grid = dof_grid_try, 
#                                            lam_grid = lam_grid_try)


covid$cases.reported.7dma <- zoo::rollmean(covid$cases.reported, k = 7, fill = NA, align = "right")
covid$cases.symptom.onset.7dma <- zoo::rollmean(covid$cases.symptom.onset, k = 7, fill = NA, align = "right")

covid$cases.reported.17dsum <- zoo::rollsum(covid$cases.reported, k = 17, fill = NA, align = "right")
covid$cases.symptom.onset.17dsum <- zoo::rollsum(covid$cases.symptom.onset, k = 17, fill = NA, align = "right")


covid$cases.reported.con <- decon.report$Chat
covid$cases.reported.decon <- decon.report$Ihat


```

Figure \@ref(fig:epi-curve-decon) shows the estimates from the deconvolution. The deconvoluted incidence curve does seem fairly well matched with the epidemic curve for symptom onset. The slight deviations observed (e.g., peak height) are likely and artifact of the spline smoothing of the data, despite the algorithm settling on `r decon.report$best_dof` degrees of freedom for the splines fit. Potentially of note is that the deconvoluted incidence curve does seem like an additional "left" transition of the symptom onset data. This could be expected as the symptom onset data is imperfect, as discussed previously. Regardless, each dataset of case counts time series will be explored in its association with the RT-qPCR data. 

```{r epi-curve-decon, echo = F, message=F, warning=F, error=F, fig.height=6*9/16*2, fig.cap = "Comparison of Epidemic Curves from Dates of Report and Symptom Onset to a Deconvoluted Incidence Curve"}



par(mar=c(3.1,3.1,2.1,1.1))
# barplot(cases.reported~date, data=covid)
barplot(cases.reported~date, data=covid, las=2, xlab = '', ylab = '', xaxt = 'n', yaxt = 'n', width = 1, space = 0, xaxs = 'i', yaxs = 'i', axes=F, col = "gainsboro", border = "gainsboro")
axis(1, at=at.points, tick=T, labels = paste(format(date.labels, "%b %y"),' '), xpd = TRUE, cex.axis=0.7, line = 0, padj = -1)
axis(2, labels = T, tick = T, cex=0.7, cex.axis = 0.7, las = 1)
title(xlab = "Date", ylab = "Cases", line = 2)
axis(3, at = at.points2, labels = format(c(min(wbe$sample_date), max(wbe$sample_date)), "%d %b %y"), tick = T, line = 0, tck = 0.02, cex.axis = 0.7)
axis(3, at = mean(at.points2), labels = "Study Period", line = 0, padj = -0.25, tck = -0.04)

barplot(cases.symptom.onset~date, data=covid, las=2, xlab = '', ylab = '', xaxt = 'n', yaxt = 'n', width = 1, space = 0, xaxs = 'i', yaxs = 'i', axes=F, add = T, col = viridis::viridis(100, alpha = 0.25)[55], border = viridis::viridis(100, alpha = 0.25)[55])


lines(seq(0.5, nrow(covid)-0.5, by=1), decon.report$Chat, lty = 3, lwd = 2)
lines(seq(0.5, nrow(covid)-0.5, by=1), decon.report$Ihat, lty = 5, lwd = 4)

# lines(seq(0.5, nrow(covid)-0.5, by=1), covid$cases.reported.7dma)
# lines(seq(0.5, nrow(covid)-0.5, by=1), covid$cases.symptom.onset.7dma)


legend("topleft", 
       bty = 'n',
       pt.bg = c("gainsboro", viridis::viridis(100, alpha = 0.25)[55], NA, NA), 
       pt.cex = 2,
       pch = c(22,22,NA,NA),
       lty = c(0, 0, 3, 5), 
       lwd = c(0, 0, 1, 2), 
       col = c("gainsboro", viridis::viridis(100, alpha = 0.25)[55], "black", "black"), 
       legend = c("Reported", "Symptom Onset", "Fitted Convolution", "Estimated Deconvolution"))

```












# It's All Clumping Together...

So, the ultimate goal of this project was to assess the predictability of COVID-19 cases given the wastewater-based epidemiological surveillance for SARS-CoV-2. Essentially, we will be regressing the case counts on the viral loads in wastewater samples. Since we have a hierarchical dataset for the RT-qPCR results, we can take a couple different approaches with the data. We could use the data at any one of the hierarchy levels, but as the unit of analysis gets smaller, the model complexity increases where at the technical replicate level we would likely need to control for the correlation among replicates of the same sample and location. Furthermore, since the data are time series, we should address potential autocorrelation in the time of the samples. If we were to take a meaningful summary of RT-qPCR results for every given sampling time, then we would effectively remove the additional sources correlation among observations, save the autocorrelation in time of course. Of note, this could contribute to a loss of information from those lower levels of data, but this may be minimal given an appropriate summarization. 

To summarize the replicates, we devised a scheme to create an aggregate measure at the facility level for each sampling time. 

1. The *technical replicates* will be summarized using an arithmetic (i.e., simple) mean.
2. The *biological replicates* will be summarized using a geometric mean. 
3. Finally, the paralleled results for the two viral sequence targets, N1 and N2, will be summarized using an arithmetic mean. 

Once at the facility level, we can include some facility level data on the total influent water / sewage volume and the total influent suspended solids concentrations that may affect the RT-qPCR results and, consequently, the predictions of COVID-19 cases. 


```{r, echo = F, message=F, warning=F, error=F}

wbe.scenarios <- list(wbe, wbe2, wbe3, wbe4, wbe5)

summarize_Replicates <- function(x){
  x %<>%
      group_by(sample_date, facility, target, biological_replicate) %>% 
      summarise(copies_per_uL = mean(copies_per_uL, na.rm=T)) %>% 
      ungroup() %>% 
      group_by(sample_date, facility, target) %>% 
      summarise(copies_per_uL = exp(mean(log(copies_per_uL), na.rm = T))) %>% 
      ungroup() %>% 
      group_by(sample_date, facility) %>% 
      summarise(copies_per_uL = sum(copies_per_uL, na.rm = T)) %>% 
      ungroup()
  return(x)
}

wbe.scenarios.facility <- lapply(wbe.scenarios, summarize_Replicates)
 

combine_Plant_data <- function(x){
  x %<>% 
      full_join(plant, by = c("sample_date"="date", "facility"="wrf")) %>%
      mutate(copies = copies_per_uL * 1e6 * influent_flow_L)
  return(x)
}

wbe.scenarios %<>% lapply(combine_Plant_data)
wbe.scenarios.facility %<>% lapply(combine_Plant_data)


summarize_Facilities <- function(x){
  x %<>%
      group_by(sample_date) %>%
      summarise(
        copies = sum(copies, na.rm = T), 
        copies_per_uL = exp(mean(log(copies_per_uL), na.rm=T)),
        mean.tss = exp(mean(log(influent_tss_mg_l), na.rm=T))
      ) %>%
      ungroup()
  return(x)
}


wbe.scenarios.date <- wbe.scenarios.facility %>% lapply(summarize_Facilities)



combine_COVID <- function(x){
  x %<>% 
      full_join(covid, by = c("sample_date"="date")) #%>%
      # filter(sample_date %in% wbe$sample_date)
  return(x)
}


wbe.scenarios %<>% lapply(combine_COVID)
wbe.scenarios.facility %<>% lapply(combine_COVID)
wbe.scenarios.date %<>% lapply(combine_COVID)


```








## All the Plots

These many plots vary with respect to their x-variables (either copy concentration, or copy number), y-variables (raw case counts, 7-day moving averages, deconvoluted case counts), and the level of the dataset (facility level or sampling day level). 


```{r all-the-plots, echo = F, message=F, warning=F, error=F, fig.height=6*9/16*3}
# plot(log(wbe.scenarios[[1]]$copies_per_uL), wbe.scenarios[[1]]$cases.reported)
# plot(log(wbe.scenarios[[1]]$copies_per_uL), wbe.scenarios[[1]]$cases.symptom.onset)
# plot(log(wbe.scenarios[[1]]$copies_per_uL), wbe.scenarios[[1]]$cases.reported.7dma)
# plot(log(wbe.scenarios[[1]]$copies_per_uL), wbe.scenarios[[1]]$cases.symptom.onset.7dma)
# plot(log(wbe.scenarios[[1]]$copies_per_uL), wbe.scenarios[[1]]$cases.reported.decon)
# 
# plot(wbe.scenarios[[1]]$cases.symptom.onset.17dsum, log(wbe.scenarios[[1]]$copies_per_uL))
# 
# 
# 
# 
# plot(log(wbe.scenarios.facility[[1]]$copies_per_uL), wbe.scenarios.facility[[1]]$cases.reported)
# plot(log(wbe.scenarios.facility[[1]]$copies_per_uL), wbe.scenarios.facility[[1]]$cases.symptom.onset)
# plot(log(wbe.scenarios.facility[[1]]$copies_per_uL), wbe.scenarios.facility[[1]]$cases.reported.7dma)
# plot(log(wbe.scenarios.facility[[1]]$copies_per_uL), wbe.scenarios.facility[[1]]$cases.symptom.onset.7dma)
# plot(log(wbe.scenarios.facility[[1]]$copies_per_uL), wbe.scenarios.facility[[1]]$cases.reported.decon)
# 
# plot(wbe.scenarios.facility[[1]]$cases.symptom.onset.17dsum, log(wbe.scenarios.facility[[1]]$copies_per_uL))
# 
# 
# plot(log(wbe.scenarios.facility[[1]]$copies), wbe.scenarios.facility[[1]]$cases.reported)
# plot(log(wbe.scenarios.facility[[1]]$copies), wbe.scenarios.facility[[1]]$cases.symptom.onset)
# plot(log(wbe.scenarios.facility[[1]]$copies), wbe.scenarios.facility[[1]]$cases.reported.7dma)
# plot(log(wbe.scenarios.facility[[1]]$copies), wbe.scenarios.facility[[1]]$cases.symptom.onset.7dma)
# plot(log(wbe.scenarios.facility[[1]]$copies), wbe.scenarios.facility[[1]]$cases.reported.decon)
# 
# plot(wbe.scenarios.facility[[1]]$cases.symptom.onset.17dsum, log(wbe.scenarios.facility[[1]]$copies))
# plot(log(wbe.scenarios.facility[[1]]$cases.symptom.onset.17dsum), log(wbe.scenarios.facility[[1]]$copies))
# 
# 
# facility <- "CC"
# facility <- "MI"
# facility <- "NO"
# 
# 
# plot(log(wbe.scenarios.facility[[1]]$copies[which(wbe.scenarios.facility[[1]]$facility==facility)]), wbe.scenarios.facility[[1]]$cases.reported[which(wbe.scenarios.facility[[1]]$facility==facility)])
# plot(log(wbe.scenarios.facility[[1]]$copies[which(wbe.scenarios.facility[[1]]$facility==facility)]), wbe.scenarios.facility[[1]]$cases.symptom.onset[which(wbe.scenarios.facility[[1]]$facility==facility)])
# plot(log(wbe.scenarios.facility[[1]]$copies[which(wbe.scenarios.facility[[1]]$facility==facility)]), wbe.scenarios.facility[[1]]$cases.reported.7dma[which(wbe.scenarios.facility[[1]]$facility==facility)])
# plot(log(wbe.scenarios.facility[[1]]$copies[which(wbe.scenarios.facility[[1]]$facility==facility)]), wbe.scenarios.facility[[1]]$cases.symptom.onset.7dma[which(wbe.scenarios.facility[[1]]$facility==facility)])
# plot(log(wbe.scenarios.facility[[1]]$copies[which(wbe.scenarios.facility[[1]]$facility==facility)]), wbe.scenarios.facility[[1]]$cases.reported.decon[which(wbe.scenarios.facility[[1]]$facility==facility)])
# 
# plot(wbe.scenarios.facility[[1]]$cases.symptom.onset.17dsum[which(wbe.scenarios.facility[[1]]$facility==facility)], log(wbe.scenarios.facility[[1]]$copies[which(wbe.scenarios.facility[[1]]$facility==facility)]))
# plot(log(wbe.scenarios.facility[[1]]$cases.symptom.onset.17dsum[which(wbe.scenarios.facility[[1]]$facility==facility)]), log(wbe.scenarios.facility[[1]]$copies[which(wbe.scenarios.facility[[1]]$facility==facility)]))
# 
# 
# 
# 
# 
# plot(log(wbe.scenarios.date[[1]]$copies), wbe.scenarios.date[[1]]$cases.reported)
# plot(log(wbe.scenarios.date[[1]]$copies), wbe.scenarios.date[[1]]$cases.symptom.onset)
# plot(log(wbe.scenarios.date[[1]]$copies), wbe.scenarios.date[[1]]$cases.reported.7dma)
# plot(log(wbe.scenarios.date[[1]]$copies), wbe.scenarios.date[[1]]$cases.symptom.onset.7dma)
# plot(log(wbe.scenarios.date[[1]]$copies), wbe.scenarios.date[[1]]$cases.reported.decon)
# 
# plot(wbe.scenarios.date[[1]]$cases.symptom.onset.17dsum, log(wbe.scenarios.date[[1]]$copies))
# plot(log(wbe.scenarios.date[[1]]$cases.symptom.onset.17dsum), log(wbe.scenarios.date[[1]]$copies))




explore.Plots <- function(level=3, aggregate=TRUE, scenario=1, xvar="copies_per_uL", yvar="cases.symptom.onset.7dma", x.transform=TRUE, y.transform=FALSE){
  if(level==1){
    the.data.list <- wbe.scenarios
    the.vars <- c(xvar, yvar, "facility", "target")
  }else{
    if(level==2){
      the.data.list <- wbe.scenarios.facility
      the.vars <- c(xvar, yvar, "facility")
    }else{
      if(level==3){
        the.data.list <- wbe.scenarios.date
        the.vars <- c(xvar, yvar)
      }
    }
  }
  
  the.data <- the.data.list[[scenario]][,the.vars]
  
  if(x.transform){
    the.data[,xvar] <- log(the.data[,xvar])
  }  
  
  if(y.transform){
    the.data[,yvar] <- log(the.data[,yvar])
  }
  
  if(aggregate){
    plot(as.matrix(the.data[,xvar]), as.matrix(the.data[,yvar]))
  }else{
    if(level==1){
      targets <- c("N1", "N2")
      facilities <- c("CC", "MI", "NO")
      combos <- expand.grid(t=targets, f=facilities)
      
      plot(as.matrix(the.data[which(the.data$target==combos$t[1] & the.data$facility==combos$f[1]),xvar]), 
           as.matrix(the.data[which(the.data$target==combos$t[1] & the.data$facility==combos$f[1]),yvar]), 
           pch = 1)
      
      for(ii in 2:nrow(combos)){
        points(as.matrix(the.data[which(the.data$target==combos$t[ii] & the.data$facility==combos$f[ii]),xvar]), 
               as.matrix(the.data[which(the.data$target==combos$t[ii] & the.data$facility==combos$f[ii]),yvar]), 
               pch = ii)
      }
      
      legend("topleft", pch = 1:ii, legend = paste(combos$t, combos$f))
      
    }else{
      if(level==2){
        
      facilities <- c("CC", "MI", "NO")
      
      plot(as.matrix(the.data[which(the.data$facility==facilities[1]),xvar]), 
           as.matrix(the.data[which(the.data$facility==facilities[1]),yvar]), 
           pch = 1)
      
      for(ii in 2:length(facilities)){
        points(as.matrix(the.data[which(the.data$facility==facilities[ii]),xvar]), 
               as.matrix(the.data[which(the.data$facility==facilities[ii]),yvar]), 
               pch = ii)
      }
      
      legend("topleft", pch = 1:ii, legend = facilities)
      }else{
        plot(as.matrix(the.data[,xvar]), as.matrix(the.data[,yvar]))
      }
    }
    
  }
  

}


xvars <- c("copies_per_uL", 
           "copies", 
           "cases.symptom.onset.17dsum")

yvars <- c("cases.reported", "cases.symptom.onset", 
           "cases.reported.7dma", "cases.symptom.onset.7dma", 
           "cases.reported.decon", 
           "copies_per_uL", "copies")

x.index <- 2
y.index <- 5
level.index <- 3

for(level.index in 2:3){
for(x.index in 1:2){
for(y.index in 1:5){
par(mfrow=c(3,2), mar = c(2.1, 2.1, 0.1, 0.1))
for(i in 1:5){
explore.Plots(level=level.index, 
              aggregate = F,
              scenario=i, 
              xvar=xvars[x.index], 
              yvar=yvars[y.index], 
              x.transform=TRUE, 
              y.transform=FALSE)
}
plot.new()
text(par('usr')[1], par('usr')[4], labels = paste(c(paste0("x = ", xvars[x.index]), paste0("y = ", yvars[y.index]), paste0("level = ", level.index)), collapse = "\n"), adj = c(0, 1))
}
}
}
```



## Autocorrelation


```{r}

my.data <- wbe.scenarios.date[[2]]

my.data %<>% mutate(log.copies = log(copies+1), 
                    log.copies.per.uL = log(copies_per_uL+1)) %>%
          arrange(sample_date)


# my.data <- my.data[-which(my.data$sample_date%in%as.Date(c("2020-11-19", "2020-11-18"))),]



acf(my.data$cases.symptom.onset)
acf(my.data$cases.symptom.onset.7dma[which(complete.cases(my.data$cases.symptom.onset.7dma))])
acf(my.data$cases.reported.decon)


```



## Cross-correlations

```{r, echo = F, message=F, warning=F, error=F}






the.lags <- -21:21
the.lags <- the.lags[-which(the.lags==0)]

the.vars <- c("cases.reported", "cases.symptom.onset", "cases.reported.7dma", "cases.symptom.onset.7dma", "cases.reported.decon")

for(ii in 1:length(the.vars)){
  for(i in 1:length(the.lags)){
    if(the.lags[i]<0){
      my.data[,paste0(the.vars[ii], ".lead.", abs(the.lags[i]))] <- lead(my.data[,the.vars[ii]], n = abs(the.lags[i]))
    }else{
      my.data[,paste0(the.vars[ii], ".lag.", abs(the.lags[i]))] <- lag(my.data[,the.vars[ii]], n = the.lags[i])
    }
  }
}




my.data %<>% filter(sample_date>=min(wbe$sample_date))


the.cors <- matrix(NA, nrow = length(the.lags)+1, ncol = length(the.vars))


for(ii in 1:length(the.vars)){
  for(i in 1:{length(the.lags)+1}){
    if(i == 1){
      the.cors[i,ii] <- cor(my.data[,"log.copies"], my.data[,the.vars[ii]], use = "pairwise.complete.obs", method = "spearman")
    }else{
      if(the.lags[i-1]<0){
        the.cors[i,ii] <- cor(my.data[,"log.copies"], my.data[,paste0(the.vars[ii], ".lead.", abs(the.lags[i-1]))], use = "pairwise.complete.obs", method = "spearman")
      }else{
        the.cors[i,ii] <- cor(my.data[,"log.copies"], my.data[,paste0(the.vars[ii], ".lag.", abs(the.lags[i-1]))], use = "pairwise.complete.obs", method = "spearman")
      }
    }
    
  }
}

the.cors <- cbind(c(0,the.lags), the.cors) %>% as.data.frame() %>% setNames(., nm = c("lag", the.vars)) %>% arrange(lag)

```



```{r cross-cor, echo = F, message=F, warning=F, error=F, fig.height=6*9/16*3, fig.cap = "Cross-correlations Between the Natural Logarithm of Total Viral Copies and COVID-19 Cases"}

par(mfrow = c(3,2))
plot(cases.reported ~ lag, data = the.cors, type = "h", xlab = "Lag in Days", ylab = "Correlation Coefficient", main = "Cases by Report Date")
segments(x0 = the.cors$lag[which.max(abs(the.cors$cases.reported))], x1 = the.cors$lag[which.max(abs(the.cors$cases.reported))], y0 = 0, y1 = the.cors$cases.reported[which.max(abs(the.cors$cases.reported))], lwd = 2)
text(par('usr')[2]+(1-par('plt')[2])*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[3]-(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     
     labels = bquote(rho == .(round(the.cors$cases.reported[which.max(abs(the.cors$cases.reported))], 3)) ~ at ~ lag ~ .(the.cors$lag[which.max(abs(the.cors$cases.reported))])), xpd = T, adj = c(1,0))

text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "A", adj = c(0,1), xpd = T, cex = 1, font = 2)

arrows(x0 = -15, x1 = -20, y0 = par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), y1 =  par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), code = 2, xpd = T, length = par('fin')[1]*0.02)
arrows(x0 = 15, x1 = 20, y0 = par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), y1 =  par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), code = 2, xpd = T, length = par('fin')[1]*0.02)

text(-15, par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), xpd = T, labels = "Past", adj = c(-0.2,0.5))
text(15, par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), xpd = T, labels = "Future", adj = c(1.2,0.5))




plot(cases.symptom.onset ~ lag, data = the.cors, type = "h", xlab = "Lag in Days", ylab = "Correlation Coefficient", main = "Cases by Symptom Onset Date")
segments(x0 = the.cors$lag[which.max(abs(the.cors$cases.symptom.onset))], x1 = the.cors$lag[which.max(abs(the.cors$cases.symptom.onset))], y0 = 0, y1 = the.cors$cases.symptom.onset[which.max(abs(the.cors$cases.symptom.onset))], lwd = 2)
text(par('usr')[2]+(1-par('plt')[2])*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[3]-(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]),
     
     labels = bquote(rho == .(round(the.cors$cases.symptom.onset[which.max(abs(the.cors$cases.symptom.onset))], 3)) ~ at ~ lag ~ .(the.cors$lag[which.max(abs(the.cors$cases.symptom.onset))])), xpd = T, adj = c(1,0))

text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "B", adj = c(0,1), xpd = T, cex = 1, font = 2)

arrows(x0 = -15, x1 = -20, y0 = par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), y1 =  par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), code = 2, xpd = T, length = par('fin')[1]*0.02)
arrows(x0 = 15, x1 = 20, y0 = par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), y1 =  par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), code = 2, xpd = T, length = par('fin')[1]*0.02)

text(-15, par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), xpd = T, labels = "Past", adj = c(-0.2,0.5))
text(15, par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), xpd = T, labels = "Future", adj = c(1.2,0.5))







plot(cases.reported.7dma ~ lag, data = the.cors, type = "h", xlab = "Lag in Days", ylab = "Correlation Coefficient", main = "7-day Moving Average of Cases by Report Date")
segments(x0 = the.cors$lag[which.max(abs(the.cors$cases.reported.7dma))], x1 = the.cors$lag[which.max(abs(the.cors$cases.reported.7dma))], y0 = 0, y1 = the.cors$cases.reported.7dma[which.max(abs(the.cors$cases.reported.7dma))], lwd = 2)
text(par('usr')[2]+(1-par('plt')[2])*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[3]-(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]),
     
     labels = bquote(rho == .(round(the.cors$cases.reported.7dma[which.max(abs(the.cors$cases.reported.7dma))], 3)) ~ at ~ lag ~ .(the.cors$lag[which.max(abs(the.cors$cases.reported.7dma))])), xpd = T, adj = c(1,0))

text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "C", adj = c(0,1), xpd = T, cex = 1, font = 2)

arrows(x0 = -15, x1 = -20, y0 = par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), y1 =  par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), code = 2, xpd = T, length = par('fin')[1]*0.02)
arrows(x0 = 15, x1 = 20, y0 = par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), y1 =  par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), code = 2, xpd = T, length = par('fin')[1]*0.02)

text(-15, par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), xpd = T, labels = "Past", adj = c(-0.2,0.5))
text(15, par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), xpd = T, labels = "Future", adj = c(1.2,0.5))






plot(cases.symptom.onset.7dma ~ lag, data = the.cors, type = "h", xlab = "Lag in Days", ylab = "Correlation Coefficient", main = "7-day Moving Average of Cases by Symptom Onset Date")
segments(x0 = the.cors$lag[which.max(abs(the.cors$cases.symptom.onset.7dma))], x1 = the.cors$lag[which.max(abs(the.cors$cases.symptom.onset.7dma))], y0 = 0, y1 = the.cors$cases.symptom.onset.7dma[which.max(abs(the.cors$cases.symptom.onset.7dma))], lwd = 2)
text(par('usr')[2]+(1-par('plt')[2])*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[3]-(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     
     labels = bquote(rho == .(round(the.cors$cases.symptom.onset.7dma[which.max(abs(the.cors$cases.symptom.onset.7dma))], 3)) ~ at ~ lag ~ .(the.cors$lag[which.max(abs(the.cors$cases.symptom.onset.7dma))])), xpd = T, adj = c(1,0))

text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "D", adj = c(0,1), xpd = T, cex = 1, font = 2)

arrows(x0 = -15, x1 = -20, y0 = par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), y1 =  par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), code = 2, xpd = T, length = par('fin')[1]*0.02)
arrows(x0 = 15, x1 = 20, y0 = par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), y1 =  par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), code = 2, xpd = T, length = par('fin')[1]*0.02)

text(-15, par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), xpd = T, labels = "Past", adj = c(-0.2,0.5))
text(15, par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), xpd = T, labels = "Future", adj = c(1.2,0.5))









plot(cases.reported.decon ~ lag, data = the.cors, type = "h", xlab = "Lag in Days", ylab = "Correlation Coefficient", main = "Deconvoluted Cases by Report Date")
segments(x0 = the.cors$lag[which.max(abs(the.cors$cases.reported.decon))], x1 = the.cors$lag[which.max(abs(the.cors$cases.reported.decon))], y0 = 0, y1 = the.cors$cases.reported.decon[which.max(abs(the.cors$cases.reported.decon))], lwd = 2)


text(par('usr')[2]+(1-par('plt')[2])*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[3]-(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     
     labels = bquote(rho == .(round(the.cors$cases.reported.decon[which.max(abs(the.cors$cases.reported.decon))], 3)) ~ at ~ lag ~ .(the.cors$lag[which.max(abs(the.cors$cases.reported.decon))])), xpd = T, adj = c(1,0))

text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "E", adj = c(0,1), xpd = T, cex = 1, font = 2)

arrows(x0 = -15, x1 = -20, y0 = par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), y1 =  par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), code = 2, xpd = T, length = par('fin')[1]*0.02)
arrows(x0 = 15, x1 = 20, y0 = par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), y1 =  par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), code = 2, xpd = T, length = par('fin')[1]*0.02)

text(-15, par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), xpd = T, labels = "Past", adj = c(-0.2,0.5))
text(15, par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), xpd = T, labels = "Future", adj = c(1.2,0.5))
```













## Exploring Model Fits



```{r}

my.data <- wbe.scenarios.date[[2]]

my.data %<>% mutate(log.copies = log(copies+1), 
                    log.copies.per.uL = log(copies_per_uL+1))

my.data <- my.data[-which(my.data$sample_date%in%as.Date(c("2020-11-19", "2020-11-18"))),]



fit1 <- lm(cases.symptom.onset ~ log.copies, data = my.data)
# plot(fit1)
summary(fit1)
plot(cases.symptom.onset ~ log.copies, data = my.data)
abline(fit1)

fit2 <- glm(cases.symptom.onset ~ log.copies, data = my.data, family = "gaussian")
# plot(fit2)
summary(fit2)
plot(cases.symptom.onset ~ log.copies, data = my.data)
abline(fit2)

fit3 <- glm(cases.symptom.onset ~ log.copies, data = my.data, family = "poisson")
# plot(fit3)
summary(fit3)
1-pchisq(fit3$deviance, fit3$df.residual)

plot(cases.symptom.onset ~ log.copies, data = my.data)
preds.fit3 <- cbind(my.data$log.copies, predict(fit3, type = "response"))
preds.fit3 <- preds.fit3[order(preds.fit3[,1]),]
lines(preds.fit3[,1], preds.fit3[,2])



# fit3 <- glm(cases.symptom.onset ~ copies, data = my.data, family = "poisson")
# # plot(fit3)
# summary(fit3)
# plot(cases.symptom.onset ~ copies, data = my.data)
# preds.fit3 <- cbind(my.data$copies, predict(fit3, type = "response"))
# preds.fit3 <- preds.fit3[order(preds.fit3[,1]),]
# lines(preds.fit3[,1], preds.fit3[,2])


fit4 <- MASS::glm.nb(cases.symptom.onset ~ log.copies, data = my.data)
# plot(fit4)
summary(fit4)

1-pchisq(fit4$deviance, fit4$df.residual)

plot(cases.symptom.onset ~ log.copies, data = my.data)
preds.fit4 <- cbind(my.data$log.copies, predict(fit4, type = "response"))
preds.fit4 <- preds.fit4[order(preds.fit4[,1]),]
lines(preds.fit4[,1], preds.fit4[,2])

```






```{r}

my.data <- wbe.scenarios.date[[2]]

my.data %<>% mutate(log.copies = log(copies+1), 
                    log.copies.per.uL = log(copies_per_uL+1))

my.data <- my.data[-which(my.data$sample_date%in%as.Date(c("2020-11-19", "2020-11-18"))),]




# fit1 <- lm(cases.symptom.onset.7dma ~ log.copies, data = my.data)
# # plot(fit1)
# summary(fit1)
# plot(cases.symptom.onset.7dma ~ log.copies, data = my.data)
# abline(fit1)

fit2 <- glm(cases.symptom.onset.7dma ~ log.copies, data = my.data, family = "gaussian")
# plot(fit2)
summary(fit2)
plot(cases.symptom.onset.7dma ~ log.copies, data = my.data)
abline(fit2)



# fit3 <- glm(cases.symptom.onset.7dma ~ log.copies, data = my.data, family = "poisson")
# # plot(fit3)
# summary(fit3)
# 1-pchisq(fit3$deviance, fit3$df.residual)
# 
# plot(cases.symptom.onset.7dma ~ log.copies, data = my.data)
# preds.fit3 <- cbind(my.data$log.copies, predict(fit3, type = "response"))
# preds.fit3 <- preds.fit3[order(preds.fit3[,1]),]
# lines(preds.fit3[,1], preds.fit3[,2])



# fit3 <- glm(cases.symptom.onset ~ copies, data = my.data, family = "poisson")
# # plot(fit3)
# summary(fit3)
# plot(cases.symptom.onset ~ copies, data = my.data)
# preds.fit3 <- cbind(my.data$copies, predict(fit3, type = "response"))
# preds.fit3 <- preds.fit3[order(preds.fit3[,1]),]
# lines(preds.fit3[,1], preds.fit3[,2])


fit4 <- MASS::glm.nb(cases.symptom.onset.7dma ~ log.copies, data = my.data)
# plot(fit4)
summary(fit4)

1-pchisq(fit4$deviance, fit4$df.residual)

plot(cases.symptom.onset.7dma ~ log.copies, data = my.data)
preds.fit4 <- cbind(my.data$log.copies, predict(fit4, type = "response"))
preds.fit4 <- preds.fit4[order(preds.fit4[,1]),]
lines(preds.fit4[,1], preds.fit4[,2])


```







```{r}

my.data <- wbe.scenarios.date[[2]]

my.data %<>% mutate(log.copies = log(copies+1), 
                    log.copies.per.uL = log(copies_per_uL+1))

my.data <- my.data[-which(my.data$sample_date%in%as.Date(c("2020-11-19", "2020-11-18"))),]




# fit1 <- lm(cases.symptom.onset.7dma ~ log.copies, data = my.data)
# # plot(fit1)
# summary(fit1)
# plot(cases.symptom.onset.7dma ~ log.copies, data = my.data)
# abline(fit1)

fit2 <- glm(cases.reported.decon ~ log.copies, data = my.data, family = "gaussian")
# plot(fit2)
summary(fit2)
plot(cases.reported.decon ~ log.copies, data = my.data)
abline(fit2)



# fit3 <- glm(cases.symptom.onset.7dma ~ log.copies, data = my.data, family = "poisson")
# # plot(fit3)
# summary(fit3)
# 1-pchisq(fit3$deviance, fit3$df.residual)
# 
# plot(cases.symptom.onset.7dma ~ log.copies, data = my.data)
# preds.fit3 <- cbind(my.data$log.copies, predict(fit3, type = "response"))
# preds.fit3 <- preds.fit3[order(preds.fit3[,1]),]
# lines(preds.fit3[,1], preds.fit3[,2])



# fit3 <- glm(cases.symptom.onset ~ copies, data = my.data, family = "poisson")
# # plot(fit3)
# summary(fit3)
# plot(cases.symptom.onset ~ copies, data = my.data)
# preds.fit3 <- cbind(my.data$copies, predict(fit3, type = "response"))
# preds.fit3 <- preds.fit3[order(preds.fit3[,1]),]
# lines(preds.fit3[,1], preds.fit3[,2])


fit4 <- MASS::glm.nb(cases.reported.decon ~ log.copies, data = my.data)
# plot(fit4)
summary(fit4)

1-pchisq(fit4$deviance, fit4$df.residual)

plot(cases.reported.decon ~ log.copies, data = my.data)
preds.fit4 <- cbind(my.data$log.copies, predict(fit4, type = "response"))
preds.fit4 <- preds.fit4[order(preds.fit4[,1]),]
lines(preds.fit4[,1], preds.fit4[,2])


```






















```{r, eval=F}
wbe_covid %<>% 
  group_by(sample_date, target, facility, biological_replicate, cases, moving_avg_cases) %>% 
  summarise(copies.lod = mean(copy_num_uL_rxn, na.rm=T), copies = mean(copies, na.rm=T))



plot(wbe_covid$sample_date[which(complete.cases(wbe_covid[,c("sample_date", "copies.lod")]) & wbe_covid$target=="N1")], log10(wbe_covid$copies.lod[which(complete.cases(wbe_covid[,c("sample_date", "copies.lod")]) & wbe_covid$target=="N1")]*20/2*25/3*60*1e6))

points(wbe_covid$sample_date[which(complete.cases(wbe_covid[,c("sample_date", "copies")]) & wbe_covid$target=="N1")], log10(wbe_covid$copies[which(complete.cases(wbe_covid[,c("sample_date", "copies")]) & wbe_covid$target=="N1")]*20/2*25/3*60*1e6), pch = 16)

abline(h = log10(2e-4*20/2*25/3*60*1e6))

abline(h = log10(exp(qqnorm.copies.n1$lod)*20/2*25/3*60*1e6))
abline(h = log10(exp(qqnorm.copies.n1$lod)*20/2*25/3*60*1e6/sqrt(2)))
abline(h = log10(exp(qqnorm.copies.n1$lod)*20/2*25/3*60*1e6)/sqrt(2))


abline(h = log10(exp(qqnorm.copies.n1$lod)*20/2*25/3*60*1e6)/2, lty=2)
abline(h = log10(exp(qqnorm.copies.n1$lod)*20/2*25/3*60*1e6/2), lty=2)

10^8

```










## Discussion

### Concepts of Sensitivity

Similar to Bayesian Framework discussed by Kyle Curtis during the Wastewater-based Epidemiology Researchers Collaboration Network Webinar on 31 March 2021.

$$Pr(+Sample~|~+RT-qPCR) = \frac{Pr(+RT-qPCR~|~+Sample) \times Pr(+Sample)}{[Pr(+RT-qPCR~|~+Sample) \times Pr(+Sample)] + [Pr(+Rt-qPCR~|~-Sample) \times Pr(-Sample)]}$$


It is important that we consider the sources of these water samples and how heterogeneity in these sources may impact downstream analyses. The wastewater reclamation facilities, as sources of water samples, may differ in  at least two potentially meaningful ways: the facility itself (e.g., size, processing, infrastructure, ...) and the origin of the water / sewage that is processed. Admittedly, the demarcation between facility and water characteristics may be impertinent as they are likely interrelated (e.g., size may reflect volume totals of influent water / sewage). However, considerations may still be warranted for their potential impacts and artifacts on data.
  
Ultimately, what characteristics of the wastewater reclamation facilities and service areas ***could*** affect water samples and, subsequently, their analysis?

Well, before considering the differences among facilities, let us first explore the sampling of water / sewage. Ideally, the collected samples would be representative of the water / sewage as a whole. What could prevent a sample from being representative? Using sparse chemistry knowledge and definitions, the water / sewage is likely more a mixture than a solution and, as such, the mixture may not be homogeneous (i.e., well mixed). So, a small sample may not be representative of the whole. The samples are, however, composites from a 24-hour period which may alleviate this as well as concerns from a cross-sectional perspective. 

With these sample and procedural impacts highlighted, the potential differences among facilities could potentially exacerbate these differences. For example, if Facility A collects larger volumes of water / sewage than Facility B, then we may expect the samples from Facility A to be drawn from a more homogeneous (well-mixed) source (I'm picturing a river with a waterfall versus a creek). There may be additional sources of heterogeneity due to a facility itself (e.g., water processing and point at which samples are taken), but at this point it is more of a thought experiment than empirical. 

Perhaps more profound in their effects on sample heterogeneity, these facilities have different service areas. The facilities effectively divide the county in three with respect to the service boundaries and the sewer networks. It is not much of a stretch to assume that the service areas differ in served population size and characteristics. So, perhaps the water samples are representative of these "subpopulations" and data analysis should consider them explicitly. Furthermore, due to potential differences in landscapes and built environments of service areas, weather events such as rainfall may be important to consider as sources of "error" in measurement. 







# Appendix

## Copies and Normality 


```{r, echo=F, eval=F}
dev.off()
plot(wbe$sample_date[which(wbe$target=="N1" & wbe$facility=="CC")], log10(wbe$copies[which(wbe$target=="N1" & wbe$facility=="CC")]*1e6), pch = as.numeric(as.factor(wbe$biological_replicate[which(wbe$target=="N1" & wbe$facility=="CC")])))

plot(wbe$sample_date[which(wbe$target=="N2" & wbe$facility=="CC")], log10(wbe$copies[which(wbe$target=="N2" & wbe$facility=="CC")]*1e6), pch = as.numeric(as.factor(wbe$biological_replicate[which(wbe$target=="N2" & wbe$facility=="CC")])))




plot(wbe$sample_date[which(wbe$target=="N1" & wbe$facility=="MI")], log10(wbe$copies[which(wbe$target=="N1" & wbe$facility=="MI")]*1e6), pch = as.numeric(as.factor(wbe$biological_replicate[which(wbe$target=="N1" & wbe$facility=="MI")])))

plot(wbe$sample_date[which(wbe$target=="N2" & wbe$facility=="MI")], log10(wbe$copies[which(wbe$target=="N2" & wbe$facility=="MI")]*1e6), pch = as.numeric(as.factor(wbe$biological_replicate[which(wbe$target=="N2" & wbe$facility=="MI")])))




plot(wbe$sample_date[which(wbe$target=="N1" & wbe$facility=="NO")], log10(wbe$copies[which(wbe$target=="N1" & wbe$facility=="NO")]*1e6), pch = as.numeric(as.factor(wbe$biological_replicate)))

plot(wbe$sample_date[which(wbe$target=="N2" & wbe$facility=="NO")], log10(wbe$copies[which(wbe$target=="N2" & wbe$facility=="NO")]*1e6), pch = as.numeric(as.factor(wbe$biological_replicate)))
```


```{r}
# hist(wbe$copy_num_uL_rxn)
# hist(wbe$copy_num_uL_rxn, breaks = 100)
# hist(wbe$copy_num_uL_rxn[which(wbe$copy_num_uL_rxn<1)])
# hist(wbe$copy_num_uL_rxn[which(wbe$copy_num_uL_rxn<1)], breaks = 100)
# hist(wbe$copy_num_uL_rxn[which(wbe$copy_num_uL_rxn<0.2)])
# hist(wbe$copy_num_uL_rxn[which(wbe$copy_num_uL_rxn<0.2)], breaks = 100)
# hist(wbe$copy_num_uL_rxn[which(wbe$copy_num_uL_rxn<0.1)])
# hist(wbe$copy_num_uL_rxn[which(wbe$copy_num_uL_rxn<0.1)], breaks = 100)
# 
# hist(wbe$copies)
# hist(log(wbe$copies))
# 
# hist(log(wbe$copy_num_uL_rxn))
# 
# summary(wbe$copy_num_uL_rxn)
# summary(wbe$copies)
# summary(wbe$copies[which(wbe$copies>min(wbe$copies, na.rm=T))])
# 
# View(wbe[order(wbe$copies),])
```


```{r, echo=F, eval=F}
# layout(matrix(c(1,2), nrow = 2), widths = c(lcm(8*2.54)), heights = c(lcm(4.5*2.54), lcm(4.5*2.54)))
# # layout.show(2)
# qqnorm.copies.n1 <- qqnorm(log(wbe$copies[which(wbe$target=="N1")]), main = "Normal Q-Q Plot for N1 log(Copies)") %>% as.data.frame()
# qqnorm.copies.n2 <- qqnorm(log(wbe$copies[which(wbe$target=="N2")]), main = "Normal Q-Q Plot for N2 log(Copies)") %>% as.data.frame()
# 
# 
# qqnorm.Explorer <- function(qqnorm.copies){
#         qqnorm.copies <- qqnorm.copies[which(complete.cases(qqnorm.copies)),]
#         qqnorm.copies <- qqnorm.copies[order(qqnorm.copies$x),]
#         qqnorm.copies <- cbind(qqnorm.copies, rbind(NA, qqnorm.copies[-nrow(qqnorm.copies),])) %>% setNames(., nm = c("x", "y", "x-1", "y-1"))
#         qqnorm.copies %<>% mutate(rise = y-`y-1`, run = x-`x-1`) %>% mutate(slope = rise / run)
#         
#         qqnorm.copies$lod <- NA
#         qqnorm.copies$loq <- NA
#         
#         prev.slope <- 1
#         lod.found <- 0
#         for(i in 2:nrow(qqnorm.copies)){
#           if(lod.found==0){
#             if(qqnorm.copies$slope[i]<1 & prev.slope <1){
#               qqnorm.copies$lod[i] <- 1
#               lod.found <- 1
#             }else{
#               prev.slope <- qqnorm.copies$slope[i]
#             }
#           }
#           if(lod.found==1){
#             if(qqnorm.copies$slope[i]>1){
#               qqnorm.copies$loq[i] <- 1
#               break
#             }else{
#               prev.slope <- qqnorm.copies$slope[i]
#             }
#           }
#         }
# 
#         lod.copies <- qqnorm.copies$y[which(qqnorm.copies$lod==1)]
#         loq.copies <- qqnorm.copies$y[which(qqnorm.copies$loq==1)]
#         
#         return(list(qqnorm.dataset = qqnorm.copies, lod = lod.copies, loq = loq.copies))
# }
# 
# qqnorm.copies.n1 <- qqnorm.Explorer(qqnorm.copies.n1)
# qqnorm.copies.n2 <- qqnorm.Explorer(qqnorm.copies.n2)
# 
# 
# 
# layout(matrix(c(1,2), nrow = 2), widths = c(lcm(8*2.54)), heights = c(lcm(4.5*2.54), lcm(4.5*2.54)))
# # layout.show(2)
# qqnorm(log(wbe$copies[which(wbe$target=="N1")]), main = "Normal Q-Q Plot for N1 log(Copies)")
# abline(h = qqnorm.copies.n1$lod)
# text(par('usr')[1], par('usr')[4], labels = paste("LOD =", round(qqnorm.copies.n1$lod,3)), adj = c(-0.05,1.2))
# abline(h = qqnorm.copies.n1$loq, lty = 3)
# text(par('usr')[1], par('usr')[4], labels = paste("LOQ =", round(qqnorm.copies.n1$loq,3)), adj = c(-0.05,2.4))
# legend("bottomright", lty = c(1,3), legend = c("LOD", "LOQ"))
# 
# 
# qqnorm(log(wbe$copies[which(wbe$target=="N2")]), main = "Normal Q-Q Plot for N2 log(Copies)")
# abline(h = qqnorm.copies.n2$lod)
# text(par('usr')[1], par('usr')[4], labels = paste("LOD =", round(qqnorm.copies.n2$lod,3)), adj = c(-0.05,1.2))
# abline(h = qqnorm.copies.n2$loq, lty = 3)
# text(par('usr')[1], par('usr')[4], labels = paste("LOQ =", round(qqnorm.copies.n2$loq,3)), adj = c(-0.05,2.4))
# legend("bottomright", lty = c(1,3), legend = c("LOD", "LOQ"))




# n1 ct --> copies
# > 10^((37.14072-34.008)/-3.3890)
# [1] 0.1190203
# > 10^((36.96446-34.008)/-3.3890)
# [1] 0.1341623

# n2 ct --> copies
# > 10^((37.09759-32.416)/-3.3084)
# [1] 0.03845372
# > 10^((36.99855-32.416)/-3.3084)
# [1] 0.04119782


# n1 copies
# > exp(lod.copies.n1)
# [1] 0.1194078
# > exp(loq.copies.n1)
# [1] 0.1379563

# n2 copies
# > exp(loq.copies.n2)
# [1] 0.04412582
# > exp(lod.copies.n2)
# [1] 0.03863563


```




```{r, echo=F, eval=F}

cns.all.qc <- qc %>% group_by(collection_num, target) %>% summarise(n=n()) %>% filter(n>=15) %>% select(collection_num, target)

qc.index <- c()

for(i in 1:nrow(cns.all.qc)){
  qc.index <- c(qc.index, which(qc$collection_num==cns.all.qc$collection_num[i] & qc$target==cns.all.qc$target[i]))
}


qc.test <- qc[qc.index,]

qc.by.target <- split(qc.test , f = qc.test$target)

qc.by.target.by.collection.number <- lapply(qc.by.target, function(x){split(x, f = x$collection_num)})

qc.standard.curves <- lapply(qc.by.target.by.collection.number, function(x){lapply(x, function(y){lm(ct~log10(quantity), data=y)})})

qc.sc.ests <- lapply(qc.standard.curves, function(x){lapply(x, function(y){c(coef(y), r2=summary(y)$r.squared)})})

qc.sc.ests <- lapply(qc.sc.ests, function(x){bind_rows(x, .id = "CN")}) %>% bind_rows(.id="target")

qc.sc.ests$Efficiency <- 10^(-1/qc.sc.ests$`log10(quantity)`)-1

qc.sc.ests <- qc.sc.ests %>% 
  group_by(target) %>% 
  summarise(
    `(Intercept)`=paste0(round(mean(`(Intercept)`), 3), " (", round(sd(`(Intercept)`),3), ")"), 
    `log10(quantity)`=paste0(round(mean(`log10(quantity)`),3), " (", round(sd(`log10(quantity)`),3), ")"), 
    r2 = paste0(round(mean(r2),3), " (", round(sd(r2),3), ")"), 
    Efficiency = paste0(round(mean(Efficiency),3), " (", round(sd(Efficiency),3), ")")
    ) %>% 
  ungroup() %>% 
  mutate(CN = "MEAN (SD)") %>% 
  bind_rows(qc.sc.ests%>%mutate_if(is.numeric, ~round(.,3))%>%mutate_all(~as.character(.)), .) %>% 
  arrange(target, CN)





as_grouped_data(qc.sc.ests, "target") %>% 
  flextable() %>% 
  border_remove() %>% 
  align(part = "all", align = "right") %>%
  align(part = "header", align = "center") %>%
  align(part = "body", j = 1, align = "left") %>%
  hline(part = "header", i = 1, border = fp_border_default(color = "black", width = 2)) %>%
  hline(part = "body", i = which(!is.na(as_grouped_data(qc.sc.ests, "target")$target)), border = fp_border_default(color = "black", width = 0.5)) %>% 
  bold(part = "header") %>%
  font(part = "all", fontname = "Arial") %>%
  fontsize(part = "header", size = 9) %>% 
  fontsize(part = "body", size = 8)%>%
  autofit()
 






qc.means <- qc %>% group_by(target, quantity) %>% summarise(n= n(), mean.ct = mean(ct), sd.ct = sd(ct))

sc.mean.n1 <- lm(mean.ct~log10(quantity), data=qc.means[which(qc.means$target=="N1"),])
sc.mean.n2 <- lm(mean.ct~log10(quantity), data=qc.means[which(qc.means$target=="N2"),])






layout(matrix(c(1,2), nrow = 2), widths = c(lcm(8*2.54)), heights = c(lcm(4.5*2.54), lcm(4.5*2.54)))
# layout.show(2)

plot(0,type='n',axes=TRUE, ylim = c(10,40), xlim = c(0,7), xlab = "log10(quantity)", ylab = "Cycle Threshold")
abline(sc.mean.n1, lwd = 10, col = "darkgrey")


for(i in 1:length(qc.standard.curves$N1)){
  abline(qc.standard.curves$N1[[i]], lty = 3)
}
abline(qc.standard.curves$N1$`13`, lwd = 4)
points(qc.by.target.by.collection.number$N1$`13`$ct~log10(qc.by.target.by.collection.number$N1$`13`$quantity), cex = 2)
text(par('usr')[2],par('usr')[4],labels = paste("13: Ct =",round(coef(qc.standard.curves$N1$`13`)[1],3), round(coef(qc.standard.curves$N1$`13`)[2],3), "*log10(quantity)"), adj = c(1, 1.2))
text(par('usr')[2],par('usr')[4],labels = paste("Mean: Ct =",round(coef(sc.mean.n1)[1],3), round(coef(sc.mean.n1)[2],3), "*log10(quantity)"), adj = c(1, 2.4))

title(main = "Standard Curves for N1")


plot(0,type='n',axes=TRUE, ylim = c(10,40), xlim = c(0,7), xlab = "log10(quantity)", ylab = "Cycle Threshold")
abline(sc.mean.n2, lwd = 10, col = "darkgrey")

for(i in 1:length(qc.standard.curves$N2)){
  abline(qc.standard.curves$N2[[i]], lty = 3)
}
abline(qc.standard.curves$N2$`24`, lwd = 4)
points(qc.by.target.by.collection.number$N2$`24`$ct~log10(qc.by.target.by.collection.number$N2$`24`$quantity), cex = 2)
text(par('usr')[2],par('usr')[4],labels = paste("24: Ct =",round(coef(qc.standard.curves$N2$`24`)[1],3), round(coef(qc.standard.curves$N2$`24`)[2],3), "*log10(quantity)"), adj = c(1, 1.2))
text(par('usr')[2],par('usr')[4],labels = paste("Mean: Ct =",round(coef(sc.mean.n2)[1],3), round(coef(sc.mean.n2)[2],3), "*log10(quantity)"), adj = c(1, 2.4))
title(main = "Standard Curves for N2")

```






```{r limits-table2, echo=F, message=F, warning=F, error=F}
wbe.summary.lod <- wbe %>% 
                        mutate(
                          ct.b.lod = ifelse(target=="N1", ct>qqnorm.ct.n1$lod, ct>qqnorm.ct.n2$lod),
                          ct.loq.lod = ifelse(target=="N1", 
                                              ct>qqnorm.ct.n1$loq & ct<=qqnorm.ct.n1$lod, 
                                              ct>qqnorm.ct.n2$loq & ct<=qqnorm.ct.n2$lod), 
                          ct.good = ifelse(target =="N1", 
                                           ct<=qqnorm.ct.n1$loq, 
                                           ct<=qqnorm.ct.n2$loq)
                        ) %>%
                        group_by(sample_date, facility, target, biological_replicate) %>% 
                        summarise(
                          n=n(), 
                          n.miss = sum(is.na(ct)), 
                          n.b.lod = sum(ct.b.lod, na.rm = T),
                          n.loq.lod = sum(ct.loq.lod, na.rm = T), 
                          n.good = sum(ct.good, na.rm = T)
                        ) %>% 
                        mutate_all(function(x){ifelse(is.nan(x), NA, x)}) %>% 
                        ungroup() %>%
  
                        group_by(sample_date, facility, target) %>% 
                        summarise(
                          n.bio = n(),
                          n.bio.miss = sum(n==n.miss),
                          n.bio.b.lod = sum(n==n.miss+n.b.lod), 
                          n.bio.loq.lod = sum(n-n.miss-n.b.lod==n.loq.lod & n.loq.lod!=0 & n.good == 0),
                          n.bio.good = sum(n==n.good),

                          n.total = sum(n), 
                          n.total.miss = sum(n.miss),
                          n.total.b.lod = sum(n.b.lod), 
                          n.total.loq.lod = sum(n.loq.lod), 
                          n.total.good = sum(n.good)
                      ) %>%
                        mutate_all(function(x){ifelse(is.nan(x), NA, x)}) %>% 
                        ungroup() %>%

                        group_by(facility, target) %>%
                        summarise(
                          n.days = n(), 
                          n.days.miss = sum(n.bio == n.bio.miss), 
                          n.days.b.lod = sum(n.bio == n.bio.b.lod), 
                          n.days.loq.lod = sum(n.bio - n.bio.b.lod == n.bio.loq.lod & n.bio.loq.lod!=0), 
                          n.days.good = sum(n.bio - n.bio.b.lod == n.bio.good & n.bio.good != 0),
                          
                          n.bio = sum(n.bio), 
                          n.bio.miss = sum(n.bio.miss), 
                          n.bio.b.lod = sum(n.bio.b.lod), 
                          n.bio.loq.lod = sum(n.bio.loq.lod), 
                          n.bio.good = sum(n.bio.good),
                          
                          n.total = sum(n.total), 
                          n.total.miss = sum(n.total.miss), 
                          n.total.b.lod = sum(n.total.b.lod), 
                          n.total.loq.lod = sum(n.total.loq.lod), 
                          n.total.good = sum(n.total.good)
                        )

wbe.lod.tr <- wbe.summary.lod %>% 
  mutate(
    total.miss = paste0(n.total.miss, " (", round(n.total.miss/n.total*100,1), ")"), 
    total.b.lod = paste0(n.total.b.lod, " (", round(n.total.b.lod/n.total*100, 2), ")"), 
    total.loq.lod = paste0(n.total.loq.lod, " (", round(n.total.loq.lod / n.total*100, 1), ")"), 
    total.good = paste0(n.total.good, " (", round(n.total.good/n.total*100,1), ")")) %>% 
  select(facility, target, total.miss, total.b.lod, total.loq.lod, total.good)




wbe.lod.bio <- wbe.summary.lod %>% 
  mutate(
    bio.miss = paste0(round(n.bio.miss/n.bio*100,1), " (", n.bio.miss, " / ", n.bio, ")"), 
    bio.b.lod = paste0(round(n.bio.b.lod/n.bio*100, 1), " (", n.bio.b.lod, " / ", n.bio, ")"), 
    bio.loq.lod = paste0(round(n.bio.loq.lod / (n.bio-n.bio.b.lod)*100, 1), " (", n.bio.loq.lod, " / ", n.bio-n.bio.b.lod, ")"), 
    bio.good = paste0(round(n.bio.good/(n.bio-n.bio.b.lod)*100,1), " (", n.bio.good, " / ", n.bio-n.bio.b.lod, ")")) %>% 
  select(facility, target, bio.miss, bio.b.lod, bio.loq.lod, bio.good)





wbe.lod.days <- wbe.summary.lod %>% 
  mutate(
    days.miss = paste0(round(n.days.miss/n.days*100,1), " (", n.days.miss, " / ", n.days, ")"), 
    days.b.lod = paste0(round(n.days.b.lod/n.days*100, 1), " (", n.days.b.lod, " / ", n.days, ")"), 
    days.loq.lod = paste0(round(n.days.loq.lod / (n.days-n.days.b.lod)*100, 1), " (", n.days.loq.lod, " / ", n.days-n.days.b.lod, ")"), 
    days.good = paste0(round(n.days.good/(n.days-n.days.b.lod)*100,1), " (", n.days.good, " / ", n.days-n.days.b.lod, ")")) %>% 
  select(facility, target, days.miss, days.b.lod, days.loq.lod, days.good)



wbe.lod.tr %<>% ungroup() %>% add_row(.before = 1) %>% add_row(.before = 1)
wbe.lod.tr[2,] <- as.list(names(wbe.lod.tr))

wbe.lod.bio %<>% ungroup() %>% add_row(.before = 1) %>% add_row(.before = 1)
wbe.lod.bio[2,] <- as.list(names(wbe.lod.bio))
names(wbe.lod.bio) <- names(wbe.lod.tr)

wbe.lod.days %<>% ungroup() %>% add_row(.before = 1) %>% add_row(.before = 1)
wbe.lod.days[2,] <- as.list(names(wbe.lod.days))
names(wbe.lod.days) <- names(wbe.lod.tr)

lod.tab <- rbind(wbe.lod.tr%>%add_row(), wbe.lod.bio%>%add_row(), wbe.lod.days)




flextable(lod.tab) %>%
  delete_part(part = "header") %>%
  border_remove() %>%
  
  border(part = "body", i = which(is.na(lod.tab$facility))[c(2,4)], border.top = fp_border_default(color = "black", width = 0.5)) %>%
  
  border(part = "body", i = c(2, which(is.na(lod.tab$facility))[c(2,4)]+2), border.bottom = fp_border_default(color = "black", width = 2)) %>% 
  
  font(part = "all", fontname = "Arial") %>%
  fontsize(part = "body", size = 8) %>%
  fontsize(part = "body", i = c(2, which(is.na(lod.tab$facility))[c(2,4)]+2), size = 9) %>%
  bold(part = "body", i = c(2, which(is.na(lod.tab$facility))[c(2,4)]+2)) %>% 
  
  merge_v(j = 1) %>% 
  
  width(j=1:2, width = 1) %>%
  width(j=3:6, width = 1.25) %>%
  
  align(part = "body", j = 3:6, align = "right") %>%
  align(part = "body", i = c(2, which(is.na(lod.tab$facility))[c(2,4)]+2), align = "center") %>%
  valign(part = "body", j = 1, valign = "top") %>% 
  
  set_caption(caption = "Distributions of Cycle Thresholds at Each Hierarchy") %>% 
  
  compose(part="body", j=1, i=c(2, which(is.na(lod.tab$facility))[c(2,4)]+2), value = as_paragraph("Wastewater Reclamation Facility")) %>% 
  compose(part="body", j=2, i=c(2, which(is.na(lod.tab$facility))[c(2,4)]+2), value = as_paragraph("Viral Sequence Target")) %>%
  compose(part="body", j=3, i=2, value = as_paragraph("Tech: Undetermined")) %>%
  compose(part="body", j=4, i=2, value = as_paragraph("Tech: Above LOD")) %>%
  compose(part="body", j=5, i=2, value = as_paragraph("Tech: Below LOD & Above LOQ")) %>%
  compose(part="body", j=6, i=2, value = as_paragraph("Tech: Below LOQ")) %>%
  
  compose(part="body", j=3, i=which(is.na(lod.tab$facility))[2]+2, value = as_paragraph("Bio: All TR Undetermined")) %>%
  compose(part="body", j=4, i=which(is.na(lod.tab$facility))[2]+2, value = as_paragraph("Bio: All TR Above LOD")) %>%
  compose(part="body", j=5, i=which(is.na(lod.tab$facility))[2]+2, value = as_paragraph("Bio: Non-Miss Below LOD & Above LOQ")) %>%
  compose(part="body", j=6, i=which(is.na(lod.tab$facility))[2]+2, value = as_paragraph("Bio: Non-Miss Below LOQ")) %>%
  
  compose(part="body", j=3, i=which(is.na(lod.tab$facility))[5]+1, value = as_paragraph("Days: All BR Undetermined")) %>%
  compose(part="body", j=4, i=which(is.na(lod.tab$facility))[5]+1, value = as_paragraph("Days: All BR Above LOD")) %>%
  compose(part="body", j=5, i=which(is.na(lod.tab$facility))[5]+1, value = as_paragraph("Days: Non-Miss Below LOD & Above LOQ")) %>%
  compose(part="body", j=6, i=which(is.na(lod.tab$facility))[5]+1, value = as_paragraph("Days: Non-Miss Below LOQ")) %>%
  
  vline(i = {which(is.na(lod.tab$facility))[2]+3}:{which(is.na(lod.tab$facility))[4]-1}, j = 4, border = fp_border_default(color = "black", width = 0.5)) %>% 
  vline(i = {which(is.na(lod.tab$facility))[4]+3}:nrow(lod.tab), j = 4, border = fp_border_default(color = "black", width = 0.5)) %>% 
  
  compose(part="body", j=1, i=1, value = as_paragraph("A")) %>%
  compose(part="body", j=1, i=which(is.na(lod.tab$facility))[2], value = as_paragraph("B")) %>%
  compose(part="body", j=1, i=which(is.na(lod.tab$facility))[4], value = as_paragraph("C")) %>%
  bold(part="body", j=1, i=c(1, which(is.na(lod.tab$facility))[c(2,4)])) %>%
  fontsize(part="body", j=1, i=c(1, which(is.na(lod.tab$facility))[c(2,4)]), size = 12)



```


The Part A column for `Tech: Below LOD & Above LOQ` shows the frequency of ct values falling between the limits of detection and quantification, a gray area with respect to distinguishing concentrations of genetic material. The Part A column for `Tech: Below LOQ` shows the frequency of ct values falling below the limit of quantification, a favorable range. The similarly names columns within Parts B and C have a bit different approach in their frequency calculations and, consequently, their interpretations. For Part B `Bio: Non-Miss Below LOD & Above LOQ`, the frequencies refer to biological replicates where ***all*** technical replicates ct values fell between the limits. The denominators given for the relative frequencies correspond to the number of biological replicates excluding the ones where all technical replicates yielded undetermined results. The `Bio: Non-Miss Below LOQ` column gives the frequencies of biological replicates for which all technical replicates yielded ct values below the limit of quantification. Note that these two instances of values either purely between the limits or below the LOQ do not account for all the biological replicates excluding the ones where all technical replicates yielded undetermined results. The Part C columns similarly give aggregations of biological replicates instead of technical replicates as in Part B. 

